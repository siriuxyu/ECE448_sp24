{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img width=\"400\" src=\"images/assignment5_splash.png\"/> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS440/ECE448 Spring 2024\n",
    "# MP07: Two-Player Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is to download this file: <a href=\"mp07.zip\">mp07.zip</a>.  It has the following content:\n",
    "\n",
    "* `main.py`. This file plays the game (python3 main.py).\n",
    "* `submitted.py`: Your homework. Edit, and then submit to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.\n",
    "* `mp07_notebook.ipynb`: This is a <a href=\"https://anaconda.org/anaconda/jupyter\">Jupyter</a> notebook to help you debug.  You can completely ignore it if you want, although you might find that it gives you useful instructions.\n",
    "* `grade.py`: Once your homework seems to be working, you can test it by typing `python grade.py`, which will run the tests in `tests/tests_visible.py`.\n",
    "* `tests/test_visible.py`: This file contains about half of the <a href=\"https://docs.python.org/3/library/unittest.html\">unit tests</a> that Gradescope will run in order to grade your homework.  If you can get a perfect score on these tests, then you should also get a perfect score on the additional hidden tests that Gradescope uses.\n",
    "* `grading_examples/`. This directory contains the JSON answer keys on which your grade is based (the visible ones). You are strongly encouraged to read these, to see what format your code should produce.\n",
    "* `chess/`, `res/`, `tools/`. These directories contain code and resources from PyChess that are necessary to run the assignment. \n",
    "* `requirements.txt`: This tells you which python packages you need to have installed, in order to run `grade.py`.  You can install all of those packages by typing `pip install -r requirements.txt` or `pip3 install -r requirements.txt`.\n",
    "* `extracredit*`: These files are for extra credit only, check the ec section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file (`mp07_notebook.ipynb`) will walk you through the whole MP, giving you instructions and debugging tips as you go.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. <a href=\"#section1\">Getting Started</a>\n",
    "1. <a href=\"#section2\">the PyChess API</a>\n",
    "1. <a href=\"#section3\">Assignment</a>\n",
    "1. <a href=\"#section4\">Extra Credit</a>\n",
    "1. <a href=\"#section5\">Submitted to Gradescope</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main.py` file will be the primary entry point for this assignment. Let’s start by running it as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "usage: main.py [-h] [--player0 {random,human,minimax,alphabeta,stochastic}]\n",
      "               [--player1 {random,human,minimax,alphabeta,stochastic}]\n",
      "               [--depth0 DEPTH0] [--depth1 DEPTH1] [--breadth0 BREADTH0]\n",
      "               [--breadth1 BREADTH1] [--loadgame LOADGAME]\n",
      "\n",
      "CS440 MP7 Chess\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --player0 {random,human,minimax,alphabeta,stochastic}\n",
      "                        Is player 0 a human, a random player, or some type of\n",
      "                        AI? (default: human)\n",
      "  --player1 {random,human,minimax,alphabeta,stochastic}\n",
      "                        Is player 1 a human, a random player, or some type of\n",
      "                        AI? (default: random)\n",
      "  --depth0 DEPTH0       Depth to which player 0 should search, if player 0 is\n",
      "                        an AI. (default: 2)\n",
      "  --depth1 DEPTH1       Depth to which player 1 should search, if player 1 is\n",
      "                        an AI. (default: 2)\n",
      "  --breadth0 BREADTH0   Breadth to which player 0 should search, if player 0\n",
      "                        is stochastic. (default: 2)\n",
      "  --breadth1 BREADTH1   Breadth to which player 1 should search, if player 1\n",
      "                        is stochastic. (default: 2)\n",
      "  --loadgame LOADGAME   Load a saved game from res/savedGames (default: None)\n",
      "/Users/siriux/anaconda3/bin/python\n",
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --help\n",
    "!which python\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will list the available options.  You will see that both player0  and player1 can  be human, or one of four types  of AI: random, minimax, alphabeta, or stochastic. The default is `--player0 human --player1 random`, because the random player is the only one already implemented.  In order to play against an \"AI\" that makes moves at random, type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/siriux/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/2023-2024 Spring/ECE 448/MP/mp07/main.py\", line 182, in <module>\n",
      "    application.run()\n",
      "  File \"/Users/siriux/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/2023-2024 Spring/ECE 448/MP/mp07/main.py\", line 118, in run\n",
      "    self.makemove(moveList[0][0], moveList[0][1])\n",
      "                  ~~~~~~~~^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a chess board pop up.  When you click on any white piece (you may need to double-click), you should see bright neon green dots centered in all of the squares to which that piece can legally move, like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"400\" src=\"images/assignment5_human.png\"/> \n",
    "</p>\n",
    "\n",
    "When you click (or double-click) on one of those green dots, your piece will move there.  Then the computer will move one of the black pieces, and it will be your turn again.\n",
    "\n",
    "If you have trouble using the mouse to play, you can debug your code by watching the computer play against itself.  For example,\n",
    "```bash\n",
    "python3 main.py --player0 random --player1 random\n",
    "```\n",
    "\n",
    "If you want to start from one of the stored game positions, you can load them as, for example:\n",
    "```bash\n",
    "python3 main.py --loadgame game1.txt\n",
    "```\n",
    "\n",
    "We will grade your submissions using `grade.py`. This file is available to you, so that you can understand how this assignment will be graded.\n",
    "\n",
    "Let’s see what happens when we run this script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 5.277s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 grade.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all of the tests raise `NotImplementedError`, because we have not yet implemented the functions `minimax` or `alphabeta` in `submitted.py`. We will do this in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. the PyChess API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chess-playing interface that we're using is based on [**PyChess**](https://github.com/pychess/pychess).  All the components of PyChess that you need are included in the assignment5.zip file, but if you want to learn more about PyChess, you are welcome to download and install it.  The standard distribution of PyChess includes a game-playing AI using the alphabeta search algorithm.  You are welcome to read their implementation to get hints for how to write your own, but note that we have changed the function signature so that if you simply cut and paste their code into your own, it will not work.\n",
    "\n",
    "You do not need to know how to play chess in order to do this assignment.  You need to know that chess is a game between two players, one with white pieces, one with black pieces.  White goes first.  Players alternate making moves until white wins, black wins, or there is a tie.  You don't need to know anything else about chess to do the assignment, though you may have more fun if you learn just a little (e.g., by playing against the computer).\n",
    "\n",
    "Though you don't need to know anything about chess, you do need to understand a few key concepts, and a few key functions, from the PyChess API.  The most important concepts are:\n",
    "\n",
    "1. **player**.  There are two players: Player 0, and Player 1.\n",
    "Player 0 plays white pieces, Player 1 black. Player 0 goes first.\n",
    "    - **side**. PyChess keeps track of whose turn it is by using a boolean called **side**:\n",
    "    `side==False` if Player 0 should play next.\n",
    "2. **move**.  A move is a 3-list: `move==[fro,to,promote]`.\n",
    "`fro` is a 2-list: `fro==[from_x,from_y]`, where `from_x` and `from_y` are each numbers between 1 and 8, specifying the starting x and y positions.  `to` is also a 2-list: `to==[to_x,to_y]`.\n",
    "`promote` is either `None` or `\"q\"`, where `q` means that you are trying to promote your piece to a queen.\n",
    "3. **board**.  A board is a 2-tuple of lists of pieces: `board==([white_piece0, white_piece1, ...], [black_piece0, black_piece1, ...])`.\n",
    "Each piece is a 3-list: `piece=[x,y,type]`.\n",
    "`x` is the x position of the piece (left-to-right, 1 to 8).\n",
    "`y` is the y  position of the piece (top-to-bottom, 1 to 8).\n",
    "`type` is a letter indicating the type of piece, which can be (`p`=pawn, `r`=rook, `n`=knight, `b`=bishop, `q`=queen, or `k`=king).  \n",
    "\n",
    "To get some better understanding, let's look at the way the board is initialized at the start of the game.  The function `chess.lib.convertMoves` starts with an initialized board, then runs forward through a series of specified moves, and gives us the resulting board.  If the series of specified moves is the empty string, then `chess.lib.convertMoves` gives us the opening board position:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should we start with the White player? False\n",
      "\n",
      "The starting board position is:\n",
      "[[[1, 7, 'p'], [2, 7, 'p'], [3, 7, 'p'],\n",
      "  [4, 7, 'p'], [5, 7, 'p'], [6, 7, 'p'],\n",
      "  [7, 7, 'p'], [8, 7, 'p'], [1, 8, 'r'],\n",
      "  [2, 8, 'n'], [3, 8, 'b'], [4, 8, 'q'],\n",
      "  [5, 8, 'k'], [6, 8, 'b'], [7, 8, 'n'],\n",
      "  [8, 8, 'r']],\n",
      " [[1, 2, 'p'], [2, 2, 'p'], [3, 2, 'p'],\n",
      "  [4, 2, 'p'], [5, 2, 'p'], [6, 2, 'p'],\n",
      "  [7, 2, 'p'], [8, 2, 'p'], [1, 1, 'r'],\n",
      "  [2, 1, 'n'], [3, 1, 'b'], [4, 1, 'q'],\n",
      "  [5, 1, 'k'], [6, 1, 'b'], [7, 1, 'n'],\n",
      "  [8, 1, 'r']]]\n"
     ]
    }
   ],
   "source": [
    "import chess.lib.utils, pprint\n",
    "\n",
    "side, board, flags = chess.lib.convertMoves(\"\")\n",
    "\n",
    "print(\"Should we start with the White player?\", side)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"The starting board position is:\")\n",
    "\n",
    "pprint.PrettyPrinter(compact=True,width=40).pprint(board)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A evaluate\n",
    "\n",
    "The function `value=evaluate(board)` returns the heuristic value of the board for the white player (thus, in the textbook's terminology, the white player is Max, the black player is Min).\n",
    "\n",
    "For example, you can find the numerical value of a board by typing:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the default board is 0.0\n",
      "If we eliminate one of black's rooks, the value is  14.0\n",
      "If we eliminate one of white's rooks, the value is  -14.0\n",
      "If the players are each missing a rook, the value is  0.0\n"
     ]
    }
   ],
   "source": [
    "import chess.lib.heuristics\n",
    "\n",
    "# Try the default board\n",
    "value = chess.lib.heuristics.evaluate(board)\n",
    "print(\"The value of the default board is\",value)\n",
    "\n",
    "# Try a board where Black is missing a rook\n",
    "board2 = [ board[0], board[1][:-1] ]\n",
    "value = chess.lib.heuristics.evaluate(board2)\n",
    "print(\"If we eliminate one of black's rooks, the value is \",value)\n",
    "\n",
    "# Try a board where White is missing a rook\n",
    "board3 = [ board[0][:-1], board[1] ]\n",
    "value = chess.lib.heuristics.evaluate(board3)\n",
    "print(\"If we eliminate one of white's rooks, the value is \",value)\n",
    "\n",
    "# Eliminate one piece from each player\n",
    "board4 = [ board[0][:-1], board[1][:-1] ]\n",
    "value = chess.lib.heuristics.evaluate(board4)\n",
    "print(\"If the players are each missing a rook, the value is \",value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B encode and decode\n",
    "\n",
    "Lists cannot be used as keys in a dict, therefore, in order to give your `moveTree` to the autograder, you will need some way to encode the moves.  `encoded=encode(*move)` converts a `move` into a string representing its standard chess encoding.  The **decode** function reverses the processing of `encode`.  For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The move [7,2]->[7,4] encodes as g7g5\n",
      "The move [5,7]->[5,8] with promotion to queen is encoded as e2e1q\n",
      "The move g7g5 is decoded to [[7, 2], [7, 4], None]\n"
     ]
    }
   ],
   "source": [
    "from chess.lib.utils import encode, decode\n",
    "\n",
    "# This statement evaluates to True\n",
    "move1 = encode([7,2],[7,4],None)\n",
    "print(\"The move [7,2]->[7,4] encodes as\",move1)\n",
    "\n",
    "# This statement also evaluates to True\n",
    "move2=encode([5,7],[5,8],\"q\")\n",
    "print(\"The move [5,7]->[5,8] with promotion to queen is encoded as\",move2)\n",
    "\n",
    "# This statement evaluates to True\n",
    "move3 = decode(\"g7g5\")\n",
    "print(\"The move g7g5 is decoded to\",move3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.C generateMoves, convertMoves, makeMove\n",
    "\n",
    "The function **generateMoves** is a [generator](https://docs.python.org/3/glossary.html#term-generator) that generates all moves that are legal on the current board.  The function **convertMoves** generates a starting board.  The function **makeMove**  implements a move, and returns the resulting board (and side and flags).  For example, the following code prints all of the moves that white can legally make, starting from the beginning board:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This move is legal now: [[1, 7], [1, 5], None] ([True, True, True, True], [1, 6])\n",
      "This move is legal now: [[1, 7], [1, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[2, 7], [2, 5], None] ([True, True, True, True], [2, 6])\n",
      "This move is legal now: [[2, 7], [2, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[3, 7], [3, 5], None] ([True, True, True, True], [3, 6])\n",
      "This move is legal now: [[3, 7], [3, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[4, 7], [4, 5], None] ([True, True, True, True], [4, 6])\n",
      "This move is legal now: [[4, 7], [4, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[5, 7], [5, 5], None] ([True, True, True, True], [5, 6])\n",
      "This move is legal now: [[5, 7], [5, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[6, 7], [6, 5], None] ([True, True, True, True], [6, 6])\n",
      "This move is legal now: [[6, 7], [6, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[7, 7], [7, 5], None] ([True, True, True, True], [7, 6])\n",
      "This move is legal now: [[7, 7], [7, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[8, 7], [8, 5], None] ([True, True, True, True], [8, 6])\n",
      "This move is legal now: [[8, 7], [8, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[2, 8], [3, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[2, 8], [1, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[7, 8], [8, 6], None] ([True, True, True, True], None)\n",
      "This move is legal now: [[7, 8], [6, 6], None] ([True, True, True, True], None)\n"
     ]
    }
   ],
   "source": [
    "import submitted, importlib\n",
    "import chess.lib\n",
    "\n",
    "# Create an initial board\n",
    "side, board, flags = chess.lib.convertMoves(\"\")\n",
    "\n",
    "# Iterate over all moves that are legal from the current  board position.  \n",
    "for move in submitted.generateMoves(board, side, flags):\n",
    "    newside, newboard, newflags = chess.lib.makeMove(side, board, move[0], move[1], flags, move[2])\n",
    "    print(\"This move is legal now:\",move, newflags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flags** and **newflags** variables specify whether or not it has become legal for black to make certain specialized types of moves.  For more information, see `chess/docs.txt`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.D random\n",
    "\n",
    "In order to help you understand the API, the file `submitted.py` contains a function from which you can copy any useful code.  The function `moveList, moveTree, value  = random(side, board, flags, chooser)` takes  the same input as the functions you  will write, and generates the same type of output, but instead of choosing a smart move, it chooses a move at random.\n",
    "\n",
    "Here, the input parameter `chooser` is set to `chooser=`<a href=\"https://docs.python.org/3/library/random.html#functions-for-sequences\">random.choice</a> during normal game play, but during grading, it will be set to some other function that selects a move in a non-random fashion.  Use this function as if it were equivalent to `random.choice`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function random in module submitted:\n",
      "\n",
      "random(board, side, flags, chooser)\n",
      "    Return a random move, resulting board, and value of the resulting board.\n",
      "    Return: (moveList, moveTree, value)\n",
      "      moveList (list): list with one element, the chosen move\n",
      "      moveTree (dict: encode(*move)->dict): a tree of moves that were evaluated in the search process\n",
      "      value (int or float): value of the board after making the chosen move\n",
      "    Input:\n",
      "      board (2-tuple of lists): current board layout, used by generateMoves and makeMove\n",
      "      side (boolean): True if player1 (Min) plays next, otherwise False\n",
      "      flags (list of flags): list of flags, used by generateMoves and makeMove\n",
      "      chooser: a function similar to random.choice, but during autograding, might not be random.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import submitted, importlib\n",
    "importlib.reload(submitted)\n",
    "help(submitted.random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, you will need to write three functions: `minimax` and `alphabeta`.  The content of these functions is described in the sections that follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A minimax search\n",
    "\n",
    "For Part 1 of this assignment, you will implement minimax search. Specifically, you will implement a function `minimax(side, board, flags, depth)` in `search.py` with the following docstring:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimax in module submitted:\n",
      "\n",
      "minimax(board, side, flags, depth)\n",
      "    Return a minimax-optimal move sequence, tree of all boards evaluated, and value of best path.\n",
      "    Return: (moveList, moveTree, value)\n",
      "      moveList (list): the minimax-optimal move sequence, as a list of moves\n",
      "      moveTree (dict: encode(*move)->dict): a tree of moves that were evaluated in the search process\n",
      "      value (float): value of the final board in the minimax-optimal move sequence\n",
      "    Input:\n",
      "      board (2-tuple of lists): current board layout, used by generateMoves and makeMove\n",
      "      side (boolean): True if player1 (Min) plays next, otherwise False\n",
      "      flags (list of flags): list of flags, used by generateMoves and makeMove\n",
      "      depth (int >=0): depth of the search (number of moves)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.minimax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you  can see, the function accepts `side`, `board`, and `flags` variables, and a non-negative integer, `depth`.  It should perform minimax search over all possible move sequences of length `depth`, and return the complete tree of evaluated moves as `moveTree`.  If `side==True`, you should choose a path through this tree that minimizes the heuristic value of the final board, knowing that your opponent will be trying to maximize value; conversely if `side==False`.  Return the resulting optimal list of moves (including moves by both white and black) as `moveList`, and the numerical value of the final board as `value`.\n",
    "\n",
    "A note about `depth`: The `depth` parameter specifies the total number of moves, including moves by both white and black.  If `depth==1` and `side==False`, then you should just find one move, from the current board, that maximizes the value of the resulting board.  If `depth==2` and `side==False`, then you should find a white move, and the immediate following black move.  If `depth==3` and `side==False`, then you should find a white, black, white sequence of moves.  For example, see [wikipedia's page on minimax](https://en.wikipedia.org/wiki/Minimax#Minimax_algorithm_with_alternate_moves) for examples and pseudo-code.\n",
    "\n",
    "You are strongly encouraged to look at the grading examples in the `grading_examples` folder, to get a better understanding of what the `minimax` function outputs should look like.  For example, the board game in `more grading_examples/minimax_game0_depth2.json` contains `value` on the first line, `moveList` on the second line, and `moveTree` on the third line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value is 10.0 \n",
      "\n",
      "moveList is [[[2, 1], [3, 3], None], [[4, 7], [4, 5], None]] \n",
      "\n",
      "moveTree is: {'a7a5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'a7a6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'b7b5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'b7b6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'c7c5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'c7c6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'd7d5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'd7d6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'e7e5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'e7e6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'f7f5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'f7f6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'g5g4': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'h7h5': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'h7h6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'b8c6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'b8a6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'f8g7': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'f8h6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'g8h6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}, 'g8f6': {'a2a4': {}, 'a2a3': {}, 'b2b4': {}, 'b2b3': {}, 'd2d4': {}, 'd2d3': {}, 'e2e4': {}, 'e2e3': {}, 'g2g4': {}, 'g2g3': {}, 'h2h4': {}, 'h2h3': {}, 'a1b1': {}, 'c3d5': {}, 'c3b1': {}, 'c3b5': {}, 'c3e4': {}, 'c3a4': {}, 'f3g1': {}, 'f3g5': {}, 'f3e5': {}, 'f3h4': {}, 'f3d4': {}, 'h1g1': {}}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, pprint\n",
    "with open(\"grading_examples/minimax_game0_depth2.json\",\"r\") as f:\n",
    "    value=json.loads(f.readline())\n",
    "    print(\"value is\",value,\"\\n\")\n",
    "    moveList=json.loads(f.readline())\n",
    "    print(\"moveList is\",moveList,\"\\n\")\n",
    "    moveTree=json.loads(f.readline())\n",
    "    print(\"moveTree is:\",moveTree,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will certainly want to implement `minimax` as a recursive function.  You will certainly want to use the function `generateMoves` to generate all moves that are legal in the current game state, and you will certainly want to use `makeMove` to find the newside, newboard, and newflags that result from making each move.  When you  get to `depth==0`, you will certainly want to use `evaluate(board)` in order to compute the heuristic value of the resulting board.\n",
    "\n",
    "Once  you  have implemented minimax, you can test it by playing against it.  If you have pygame installed, the following line should pop up a board on which you can play against your own minimax player.  If you have not yet written `minimax`, the game will throw a `NotImplementedError` when it is white's turn to move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --player1 minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it is working correctly by moving one of your knights forward.  The computer should respond by moving one of its knights foward, as shown here:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"400\" src=\"images/assignment5_figure1.png\"/> \n",
    "</p>\n",
    "\n",
    "If you want to watch a minimax agent win against a random-move agent, you can type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --player0 minimax --player1 random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B alphabeta search\n",
    "\n",
    "For Part 2 of this assignment, you will implement alphabeta search. Specifically, you will implement a function `alphabeta(side, board, flags, depth)` in `search.py` with the following docstring:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function alphabeta in module submitted:\n",
      "\n",
      "alphabeta(board, side, flags, depth, alpha=-inf, beta=inf)\n",
      "    Return minimax-optimal move sequence, and a tree that exhibits alphabeta pruning.\n",
      "    Return: (moveList, moveTree, value)\n",
      "      moveList (list): the minimax-optimal move sequence, as a list of moves\n",
      "      moveTree (dict: encode(*move)->dict): a tree of moves that were evaluated in the search process\n",
      "      value (float): value of the final board in the minimax-optimal move sequence\n",
      "    Input:\n",
      "      board (2-tuple of lists): current board layout, used by generateMoves and makeMove\n",
      "      side (boolean): True if player1 (Min) plays next, otherwise False\n",
      "      flags (list of flags): list of flags, used by generateMoves and makeMove\n",
      "      depth (int >=0): depth of the search (number of moves)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import submitted\n",
    "help(submitted.alphabeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any given input board, this function should return exactly the same value and moveList as `minimax`; the only difference between the two functions will be the returned `moveTree`.  The tree returned by `alphabeta` should have fewer leaf nodes than the one returned by `minimax`, because alphabeta pruning should make it unnecessary to evaluate some of the leaf nodes.\n",
    "\n",
    "You can test this using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --player0 random --player1 alphabeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Extra Credit\n",
    "\n",
    "The heuristic we've been using, until now, is the default PyChess heuristic: it assigns a value to each piece, with extra points added or subtracted depending on the piece's location. For extra credit, if you wish, you can try to train a neural network to compute a better heuristic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.A. The Game\n",
    "\n",
    "The extra credit assignment will be graded based on how  often  your heuristic beats the default PhChess heuristic in a two-player game.\n",
    "\n",
    "Ideally, the game would be chess.  Unfortunately, grading your heuristic based on complete chess games would take too much time.  Instead, the function `extracredit_grade.py` plays a very simple game:\n",
    "\n",
    "1. Each of the two players is given the same chess board.  Each of you assigns a numerical value to the board.\n",
    "2. Then, the scoring program finds the depth-two minimax value of the same board.  This value is provided in the lists called \"values\" in the data files `extracredit_train.txt` and `extracredit_validation.txt`; but if you have already completed the main assignment, it should be the same value that you'd get by running your  `minimax` or `alphabeta` search with `depth=2`.\n",
    "3. The winner of the game is the player whose computed value is closest to the reference value.\n",
    "\n",
    "Notice that what we're asking you to do, basically, is to create a neural network that can guess the value of the PyChess heuristic two steps ahead.\n",
    "\n",
    "Notice that, if you can design a funny neural net architecture that, instead of being trained to solve this problem, solves it without training by exactly computing a two-step minimax operation, then  you're done.  This is explicitly allowed, because we think it would be a very effective and very interesting solution.\n",
    "\n",
    "Most of you, we guess, will choose a more general neural net architecture, and train it so that it imitates the results of two-step minimax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.B. Distributed Code: Exactly Reproduce the PyChess Heuristic\n",
    "\n",
    "You will find the following files for the extra credit:\n",
    "- extracredit.py: Trains the model.  This is the code you will edit and submit.\n",
    "- extracredit_embedding.py: Embeds a chess board into a (15x8x8) binary pytorch tensor.\n",
    "- extracredit_train.txt: Training data: sequences of moves, and corresponding values.\n",
    "- extracredit_validation.txt: Validation data: sequences of moves, and corresponding values.\n",
    "- extracredit_grade.py: The grading script.\n",
    "\n",
    "Try the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "extracredit_train.txt loaded board number 10000...\n",
      "/Users/siriux/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/2023-2024 Spring/ECE 448/MP/mp07/extracredit_embedding.py:107: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400235349/work/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  self.embeddings = torch.tensor(embeddings,dtype=DTYPE,device=DEVICE)\n",
      "epoch 0 training_loss 5.007500171661377 validation_loss 105.54910278320312\n",
      "epoch 0 training_loss 5.67922830581665 validation_loss 105.4651870727539\n",
      "epoch 0 training_loss 4.887976169586182 validation_loss 105.34516906738281\n",
      "epoch 0 training_loss 5.503598213195801 validation_loss 105.21501922607422\n",
      "epoch 0 training_loss 5.083850383758545 validation_loss 105.08786010742188\n",
      "epoch 0 training_loss 5.606813907623291 validation_loss 104.93092346191406\n",
      "epoch 0 training_loss 5.1201324462890625 validation_loss 104.7994155883789\n",
      "epoch 0 training_loss 4.978158950805664 validation_loss 104.70172882080078\n",
      "epoch 0 training_loss 4.755140781402588 validation_loss 104.67560577392578\n",
      "epoch 0 training_loss 5.186110973358154 validation_loss 104.61648559570312\n",
      "epoch 0 training_loss 4.915466785430908 validation_loss 104.49856567382812\n",
      "epoch 0 training_loss 5.223293304443359 validation_loss 104.45417022705078\n",
      "epoch 0 training_loss 4.999243259429932 validation_loss 104.33787536621094\n",
      "epoch 0 training_loss 5.3139777183532715 validation_loss 104.31282043457031\n",
      "epoch 0 training_loss 4.960071563720703 validation_loss 104.331298828125\n",
      "epoch 0 training_loss 5.068788051605225 validation_loss 104.29171752929688\n",
      "epoch 1 training_loss 5.005727767944336 validation_loss 104.24251556396484\n",
      "epoch 1 training_loss 4.796439170837402 validation_loss 104.25057983398438\n",
      "epoch 1 training_loss 5.3704986572265625 validation_loss 104.23442840576172\n",
      "epoch 1 training_loss 5.081935405731201 validation_loss 104.19429016113281\n",
      "epoch 1 training_loss 5.385066509246826 validation_loss 104.18941497802734\n",
      "epoch 1 training_loss 5.344119071960449 validation_loss 104.12884521484375\n",
      "epoch 1 training_loss 5.2358174324035645 validation_loss 104.10350036621094\n",
      "epoch 1 training_loss 5.00444221496582 validation_loss 104.09861755371094\n",
      "epoch 1 training_loss 4.9553751945495605 validation_loss 104.09657287597656\n",
      "epoch 1 training_loss 5.215898036956787 validation_loss 104.09210205078125\n",
      "epoch 1 training_loss 5.217381000518799 validation_loss 104.08331298828125\n",
      "epoch 1 training_loss 4.881896495819092 validation_loss 104.0890884399414\n",
      "epoch 1 training_loss 5.316316604614258 validation_loss 104.02175903320312\n",
      "epoch 1 training_loss 4.971530437469482 validation_loss 104.02735900878906\n",
      "epoch 1 training_loss 5.1216325759887695 validation_loss 104.0023193359375\n",
      "epoch 1 training_loss 5.341554164886475 validation_loss 103.94271850585938\n",
      "epoch 2 training_loss 5.601790428161621 validation_loss 103.9048080444336\n",
      "epoch 2 training_loss 5.018168926239014 validation_loss 103.84918212890625\n",
      "epoch 2 training_loss 5.165035247802734 validation_loss 103.81893920898438\n",
      "epoch 2 training_loss 5.13392972946167 validation_loss 103.78943634033203\n",
      "epoch 2 training_loss 5.112434387207031 validation_loss 103.67800903320312\n",
      "epoch 2 training_loss 5.321681976318359 validation_loss 103.72257995605469\n",
      "epoch 2 training_loss 5.086182594299316 validation_loss 103.68851470947266\n",
      "epoch 2 training_loss 5.051534175872803 validation_loss 103.67729187011719\n",
      "epoch 2 training_loss 5.140750408172607 validation_loss 103.67412567138672\n",
      "epoch 2 training_loss 5.161857604980469 validation_loss 103.67250061035156\n",
      "epoch 2 training_loss 4.874470233917236 validation_loss 103.66765594482422\n",
      "epoch 2 training_loss 5.408121585845947 validation_loss 103.62895202636719\n",
      "epoch 2 training_loss 4.883339881896973 validation_loss 103.61590576171875\n",
      "epoch 2 training_loss 4.943141460418701 validation_loss 103.60012817382812\n",
      "epoch 2 training_loss 5.037497043609619 validation_loss 103.61106872558594\n",
      "epoch 2 training_loss 5.130434989929199 validation_loss 103.69231414794922\n",
      "epoch 3 training_loss 5.115548610687256 validation_loss 103.6508560180664\n",
      "epoch 3 training_loss 4.8145294189453125 validation_loss 103.6536865234375\n",
      "epoch 3 training_loss 4.949885845184326 validation_loss 103.62380981445312\n",
      "epoch 3 training_loss 5.087531566619873 validation_loss 103.61373138427734\n",
      "epoch 3 training_loss 5.086371898651123 validation_loss 103.54256439208984\n",
      "epoch 3 training_loss 5.066354751586914 validation_loss 103.5159683227539\n",
      "epoch 3 training_loss 5.593262195587158 validation_loss 103.48947143554688\n",
      "epoch 3 training_loss 4.6902971267700195 validation_loss 103.50910949707031\n",
      "epoch 3 training_loss 5.175601482391357 validation_loss 103.53864288330078\n",
      "epoch 3 training_loss 5.076394557952881 validation_loss 103.46562957763672\n",
      "epoch 3 training_loss 5.381636142730713 validation_loss 103.45344543457031\n",
      "epoch 3 training_loss 5.061742305755615 validation_loss 103.48194885253906\n",
      "epoch 3 training_loss 5.337942123413086 validation_loss 103.44847869873047\n",
      "epoch 3 training_loss 5.4719109535217285 validation_loss 103.39508056640625\n",
      "epoch 3 training_loss 4.9678192138671875 validation_loss 103.39463806152344\n",
      "epoch 3 training_loss 5.174448013305664 validation_loss 103.41888427734375\n",
      "epoch 4 training_loss 5.21798849105835 validation_loss 103.37250518798828\n",
      "epoch 4 training_loss 5.347095012664795 validation_loss 103.3292007446289\n",
      "epoch 4 training_loss 5.253796577453613 validation_loss 103.29623413085938\n",
      "epoch 4 training_loss 5.17632532119751 validation_loss 103.32998657226562\n",
      "epoch 4 training_loss 4.773096084594727 validation_loss 103.31802368164062\n",
      "epoch 4 training_loss 5.094669818878174 validation_loss 103.25204467773438\n",
      "epoch 4 training_loss 5.158568859100342 validation_loss 103.21754455566406\n",
      "epoch 4 training_loss 4.954622268676758 validation_loss 103.22107696533203\n",
      "epoch 4 training_loss 5.227043151855469 validation_loss 103.21946716308594\n",
      "epoch 4 training_loss 5.118345737457275 validation_loss 103.22659301757812\n",
      "epoch 4 training_loss 5.296380043029785 validation_loss 103.23245239257812\n",
      "epoch 4 training_loss 5.018603324890137 validation_loss 103.20415496826172\n",
      "epoch 4 training_loss 4.9241108894348145 validation_loss 103.17115783691406\n",
      "epoch 4 training_loss 5.102371692657471 validation_loss 103.18843078613281\n",
      "epoch 4 training_loss 5.176263332366943 validation_loss 103.14981079101562\n",
      "epoch 4 training_loss 5.146413326263428 validation_loss 103.13170623779297\n",
      "epoch 5 training_loss 5.115227699279785 validation_loss 103.09016418457031\n",
      "epoch 5 training_loss 5.476624488830566 validation_loss 103.03983306884766\n",
      "epoch 5 training_loss 5.059925556182861 validation_loss 103.15789031982422\n",
      "epoch 5 training_loss 4.7799153327941895 validation_loss 103.08710479736328\n",
      "epoch 5 training_loss 5.419214725494385 validation_loss 103.06005096435547\n",
      "epoch 5 training_loss 5.191725254058838 validation_loss 103.12366485595703\n",
      "epoch 5 training_loss 5.185421466827393 validation_loss 103.08045196533203\n",
      "epoch 5 training_loss 4.881592750549316 validation_loss 103.16619110107422\n",
      "epoch 5 training_loss 4.922255516052246 validation_loss 103.1448974609375\n",
      "epoch 5 training_loss 4.880451202392578 validation_loss 103.12100982666016\n",
      "epoch 5 training_loss 5.0422539710998535 validation_loss 103.14359283447266\n",
      "epoch 5 training_loss 5.1141276359558105 validation_loss 103.12590026855469\n",
      "epoch 5 training_loss 5.310212135314941 validation_loss 103.13298797607422\n",
      "epoch 5 training_loss 5.565462589263916 validation_loss 103.11723327636719\n",
      "epoch 5 training_loss 4.814774036407471 validation_loss 103.11677551269531\n",
      "epoch 5 training_loss 5.2532734870910645 validation_loss 103.15287780761719\n",
      "epoch 6 training_loss 5.198581695556641 validation_loss 103.106201171875\n",
      "epoch 6 training_loss 5.229137420654297 validation_loss 103.05924224853516\n",
      "epoch 6 training_loss 5.3298211097717285 validation_loss 103.1111831665039\n",
      "epoch 6 training_loss 5.044286727905273 validation_loss 103.0879898071289\n",
      "epoch 6 training_loss 4.821629047393799 validation_loss 103.13874816894531\n",
      "epoch 6 training_loss 5.283874988555908 validation_loss 103.11445617675781\n",
      "epoch 6 training_loss 5.220363616943359 validation_loss 103.10130310058594\n",
      "epoch 6 training_loss 5.3260650634765625 validation_loss 103.04786682128906\n",
      "epoch 6 training_loss 5.1753830909729 validation_loss 103.05099487304688\n",
      "epoch 6 training_loss 5.0663838386535645 validation_loss 103.07426452636719\n",
      "epoch 6 training_loss 5.129546642303467 validation_loss 103.09262084960938\n",
      "epoch 6 training_loss 4.798142910003662 validation_loss 103.11502838134766\n",
      "epoch 6 training_loss 4.9902119636535645 validation_loss 103.1461181640625\n",
      "epoch 6 training_loss 5.208104610443115 validation_loss 103.07069396972656\n",
      "epoch 6 training_loss 5.139907360076904 validation_loss 103.11613464355469\n",
      "epoch 6 training_loss 4.679385662078857 validation_loss 103.07760620117188\n",
      "epoch 7 training_loss 5.26192569732666 validation_loss 103.10416412353516\n",
      "epoch 7 training_loss 5.287912368774414 validation_loss 103.10537719726562\n",
      "epoch 7 training_loss 4.891308784484863 validation_loss 103.11603546142578\n",
      "epoch 7 training_loss 4.947430610656738 validation_loss 103.07585906982422\n",
      "epoch 7 training_loss 5.166594505310059 validation_loss 103.05095672607422\n",
      "epoch 7 training_loss 4.937767505645752 validation_loss 103.09286499023438\n",
      "epoch 7 training_loss 5.112688064575195 validation_loss 103.0775375366211\n",
      "epoch 7 training_loss 4.874861240386963 validation_loss 103.07723236083984\n",
      "epoch 7 training_loss 5.150972843170166 validation_loss 103.07278442382812\n",
      "epoch 7 training_loss 5.270728588104248 validation_loss 103.11627960205078\n",
      "epoch 7 training_loss 5.106723308563232 validation_loss 103.09989166259766\n",
      "epoch 7 training_loss 5.363621234893799 validation_loss 103.07398223876953\n",
      "epoch 7 training_loss 5.212204456329346 validation_loss 103.01294708251953\n",
      "epoch 7 training_loss 5.101632595062256 validation_loss 103.03199005126953\n",
      "epoch 7 training_loss 5.1463398933410645 validation_loss 103.02729797363281\n",
      "epoch 7 training_loss 4.923134803771973 validation_loss 103.1058578491211\n",
      "epoch 8 training_loss 5.194071292877197 validation_loss 103.12197875976562\n",
      "epoch 8 training_loss 5.3333420753479 validation_loss 103.05889892578125\n",
      "epoch 8 training_loss 4.910693645477295 validation_loss 103.02787780761719\n",
      "epoch 8 training_loss 5.047708034515381 validation_loss 103.05073547363281\n",
      "epoch 8 training_loss 5.133212089538574 validation_loss 103.07689666748047\n",
      "epoch 8 training_loss 4.891838073730469 validation_loss 103.04682159423828\n",
      "epoch 8 training_loss 4.842448711395264 validation_loss 103.07105255126953\n",
      "epoch 8 training_loss 5.195054054260254 validation_loss 103.04424285888672\n",
      "epoch 8 training_loss 5.332396507263184 validation_loss 103.11222839355469\n",
      "epoch 8 training_loss 5.22237491607666 validation_loss 103.07615661621094\n",
      "epoch 8 training_loss 5.210684299468994 validation_loss 103.06123352050781\n",
      "epoch 8 training_loss 5.247089385986328 validation_loss 103.0418930053711\n",
      "epoch 8 training_loss 5.146142482757568 validation_loss 103.08662414550781\n",
      "epoch 8 training_loss 5.067465782165527 validation_loss 103.05265808105469\n",
      "epoch 8 training_loss 5.063314914703369 validation_loss 103.07268524169922\n",
      "epoch 8 training_loss 4.8461833000183105 validation_loss 103.08197021484375\n",
      "epoch 9 training_loss 5.356713771820068 validation_loss 103.06246185302734\n",
      "epoch 9 training_loss 5.137246608734131 validation_loss 103.02196502685547\n",
      "epoch 9 training_loss 4.613690376281738 validation_loss 103.09080505371094\n",
      "epoch 9 training_loss 5.020191192626953 validation_loss 103.09288787841797\n",
      "epoch 9 training_loss 4.898315906524658 validation_loss 103.0998306274414\n",
      "epoch 9 training_loss 4.877859115600586 validation_loss 103.11036682128906\n",
      "epoch 9 training_loss 5.308042526245117 validation_loss 103.06258392333984\n",
      "epoch 9 training_loss 5.2449564933776855 validation_loss 103.05176544189453\n",
      "epoch 9 training_loss 5.44364070892334 validation_loss 102.98861694335938\n",
      "epoch 9 training_loss 5.270834445953369 validation_loss 103.0431900024414\n",
      "epoch 9 training_loss 5.142644882202148 validation_loss 103.06675720214844\n",
      "epoch 9 training_loss 4.912483215332031 validation_loss 103.05921936035156\n",
      "epoch 9 training_loss 5.4267168045043945 validation_loss 103.03861236572266\n",
      "epoch 9 training_loss 4.789712905883789 validation_loss 103.03366088867188\n",
      "epoch 9 training_loss 5.255000114440918 validation_loss 102.98805236816406\n",
      "epoch 9 training_loss 5.122420310974121 validation_loss 102.94766235351562\n",
      "epoch 10 training_loss 4.881937503814697 validation_loss 102.94047546386719\n",
      "epoch 10 training_loss 4.805366039276123 validation_loss 102.95046997070312\n",
      "epoch 10 training_loss 5.182170867919922 validation_loss 102.98641967773438\n",
      "epoch 10 training_loss 4.814037322998047 validation_loss 103.05465698242188\n",
      "epoch 10 training_loss 5.161013603210449 validation_loss 103.03203582763672\n",
      "epoch 10 training_loss 5.129814624786377 validation_loss 103.04537963867188\n",
      "epoch 10 training_loss 4.9250898361206055 validation_loss 103.04353332519531\n",
      "epoch 10 training_loss 4.921568393707275 validation_loss 103.07872009277344\n",
      "epoch 10 training_loss 5.1775736808776855 validation_loss 103.0598373413086\n",
      "epoch 10 training_loss 5.300732135772705 validation_loss 102.99884033203125\n",
      "epoch 10 training_loss 4.9434099197387695 validation_loss 103.03213500976562\n",
      "epoch 10 training_loss 5.287088394165039 validation_loss 103.04828643798828\n",
      "epoch 10 training_loss 5.3610405921936035 validation_loss 103.03765869140625\n",
      "epoch 10 training_loss 5.179540157318115 validation_loss 103.04475402832031\n",
      "epoch 10 training_loss 5.564877033233643 validation_loss 103.02735900878906\n",
      "epoch 10 training_loss 5.20999813079834 validation_loss 102.95213317871094\n",
      "epoch 11 training_loss 4.743940830230713 validation_loss 103.03102111816406\n",
      "epoch 11 training_loss 4.8685808181762695 validation_loss 103.0455551147461\n",
      "epoch 11 training_loss 5.0837860107421875 validation_loss 102.9981460571289\n",
      "epoch 11 training_loss 5.347375869750977 validation_loss 103.01759338378906\n",
      "epoch 11 training_loss 5.289549350738525 validation_loss 102.9617919921875\n",
      "epoch 11 training_loss 5.0616583824157715 validation_loss 102.96479034423828\n",
      "epoch 11 training_loss 5.264991283416748 validation_loss 102.95472717285156\n",
      "epoch 11 training_loss 5.449093818664551 validation_loss 102.97517395019531\n",
      "epoch 11 training_loss 5.265536308288574 validation_loss 103.00202941894531\n",
      "epoch 11 training_loss 5.304165363311768 validation_loss 103.04296875\n",
      "epoch 11 training_loss 5.152217864990234 validation_loss 103.0276870727539\n",
      "epoch 11 training_loss 5.207947731018066 validation_loss 103.0003433227539\n",
      "epoch 11 training_loss 4.67334508895874 validation_loss 103.08931732177734\n",
      "epoch 11 training_loss 5.0377421379089355 validation_loss 103.09661102294922\n",
      "epoch 11 training_loss 4.9667534828186035 validation_loss 103.10381317138672\n",
      "epoch 11 training_loss 4.946410655975342 validation_loss 103.0372314453125\n",
      "epoch 12 training_loss 5.042776107788086 validation_loss 103.09162139892578\n",
      "epoch 12 training_loss 5.245438575744629 validation_loss 103.09550476074219\n",
      "epoch 12 training_loss 5.343134880065918 validation_loss 103.04639434814453\n",
      "epoch 12 training_loss 4.913552284240723 validation_loss 103.02865600585938\n",
      "epoch 12 training_loss 5.549208641052246 validation_loss 103.00027465820312\n",
      "epoch 12 training_loss 5.325222015380859 validation_loss 102.9348373413086\n",
      "epoch 12 training_loss 5.407413005828857 validation_loss 102.98389434814453\n",
      "epoch 12 training_loss 4.845808506011963 validation_loss 103.00471496582031\n",
      "epoch 12 training_loss 5.111367702484131 validation_loss 102.9527816772461\n",
      "epoch 12 training_loss 4.902887344360352 validation_loss 102.98355102539062\n",
      "epoch 12 training_loss 4.916943550109863 validation_loss 102.91161346435547\n",
      "epoch 12 training_loss 4.71269416809082 validation_loss 102.97952270507812\n",
      "epoch 12 training_loss 5.287186622619629 validation_loss 102.99331665039062\n",
      "epoch 12 training_loss 5.282865524291992 validation_loss 103.01517486572266\n",
      "epoch 12 training_loss 4.847338676452637 validation_loss 103.05155181884766\n",
      "epoch 12 training_loss 4.844734191894531 validation_loss 103.06989288330078\n",
      "epoch 13 training_loss 5.291617393493652 validation_loss 103.0908203125\n",
      "epoch 13 training_loss 5.087050914764404 validation_loss 103.10196685791016\n",
      "epoch 13 training_loss 4.844107151031494 validation_loss 103.11466979980469\n",
      "epoch 13 training_loss 5.511781692504883 validation_loss 103.07022094726562\n",
      "epoch 13 training_loss 5.2950849533081055 validation_loss 103.0024185180664\n",
      "epoch 13 training_loss 4.857874870300293 validation_loss 102.93795776367188\n",
      "epoch 13 training_loss 4.915285587310791 validation_loss 102.94451904296875\n",
      "epoch 13 training_loss 5.167558670043945 validation_loss 102.96118927001953\n",
      "epoch 13 training_loss 4.870199203491211 validation_loss 103.02883911132812\n",
      "epoch 13 training_loss 5.021914482116699 validation_loss 102.9991683959961\n",
      "epoch 13 training_loss 5.44024658203125 validation_loss 102.93232727050781\n",
      "epoch 13 training_loss 5.382096767425537 validation_loss 102.96365356445312\n",
      "epoch 13 training_loss 5.057046890258789 validation_loss 102.96578979492188\n",
      "epoch 13 training_loss 4.892813205718994 validation_loss 102.97679901123047\n",
      "epoch 13 training_loss 4.8753461837768555 validation_loss 102.9977798461914\n",
      "epoch 13 training_loss 5.340090751647949 validation_loss 103.01260375976562\n",
      "epoch 14 training_loss 4.828597545623779 validation_loss 103.04246520996094\n",
      "epoch 14 training_loss 5.1125054359436035 validation_loss 103.0206527709961\n",
      "epoch 14 training_loss 4.915541172027588 validation_loss 103.0436019897461\n",
      "epoch 14 training_loss 5.216696739196777 validation_loss 102.96111297607422\n",
      "epoch 14 training_loss 5.104229927062988 validation_loss 102.96509552001953\n",
      "epoch 14 training_loss 5.181305408477783 validation_loss 102.97372436523438\n",
      "epoch 14 training_loss 4.887491226196289 validation_loss 102.99635314941406\n",
      "epoch 14 training_loss 5.106041431427002 validation_loss 102.95528411865234\n",
      "epoch 14 training_loss 5.027190208435059 validation_loss 102.97161102294922\n",
      "epoch 14 training_loss 5.394527435302734 validation_loss 102.96731567382812\n",
      "epoch 14 training_loss 5.271957874298096 validation_loss 102.99923706054688\n",
      "epoch 14 training_loss 5.25896692276001 validation_loss 102.98079681396484\n",
      "epoch 14 training_loss 4.917585372924805 validation_loss 102.96925354003906\n",
      "epoch 14 training_loss 5.1554388999938965 validation_loss 102.93043518066406\n",
      "epoch 14 training_loss 5.113120079040527 validation_loss 102.9690933227539\n",
      "epoch 14 training_loss 5.329535484313965 validation_loss 102.99365997314453\n",
      "epoch 15 training_loss 5.149199962615967 validation_loss 102.98973083496094\n",
      "epoch 15 training_loss 5.220612525939941 validation_loss 102.9654312133789\n",
      "epoch 15 training_loss 4.931094169616699 validation_loss 102.93190002441406\n",
      "epoch 15 training_loss 4.947681903839111 validation_loss 102.95365905761719\n",
      "epoch 15 training_loss 5.047652721405029 validation_loss 102.99765014648438\n",
      "epoch 15 training_loss 5.472087860107422 validation_loss 102.9346694946289\n",
      "epoch 15 training_loss 4.904570579528809 validation_loss 102.94232940673828\n",
      "epoch 15 training_loss 5.09738826751709 validation_loss 102.99497985839844\n",
      "epoch 15 training_loss 5.297305107116699 validation_loss 102.92188262939453\n",
      "epoch 15 training_loss 4.96678352355957 validation_loss 102.91814422607422\n",
      "epoch 15 training_loss 4.9597272872924805 validation_loss 102.92393493652344\n",
      "epoch 15 training_loss 5.189743518829346 validation_loss 103.00545501708984\n",
      "epoch 15 training_loss 5.413457870483398 validation_loss 102.96619415283203\n",
      "epoch 15 training_loss 5.304902076721191 validation_loss 102.9395751953125\n",
      "epoch 15 training_loss 4.907665729522705 validation_loss 102.97135925292969\n",
      "epoch 15 training_loss 4.495954513549805 validation_loss 103.05123138427734\n",
      "epoch 16 training_loss 5.104507923126221 validation_loss 103.05577087402344\n",
      "epoch 16 training_loss 5.033873558044434 validation_loss 103.03306579589844\n",
      "epoch 16 training_loss 5.293665885925293 validation_loss 103.04423522949219\n",
      "epoch 16 training_loss 4.87944221496582 validation_loss 102.9601821899414\n",
      "epoch 16 training_loss 5.2763495445251465 validation_loss 102.9556655883789\n",
      "epoch 16 training_loss 5.4872822761535645 validation_loss 102.91068267822266\n",
      "epoch 16 training_loss 5.132078170776367 validation_loss 102.92131042480469\n",
      "epoch 16 training_loss 4.865663051605225 validation_loss 102.99293518066406\n",
      "epoch 16 training_loss 5.242347717285156 validation_loss 103.00566101074219\n",
      "epoch 16 training_loss 5.069413661956787 validation_loss 102.99636840820312\n",
      "epoch 16 training_loss 5.075334548950195 validation_loss 102.96219635009766\n",
      "epoch 16 training_loss 5.095014572143555 validation_loss 102.91254425048828\n",
      "epoch 16 training_loss 4.903787136077881 validation_loss 102.9625473022461\n",
      "epoch 16 training_loss 4.939679145812988 validation_loss 102.949951171875\n",
      "epoch 16 training_loss 4.938095569610596 validation_loss 102.96320343017578\n",
      "epoch 16 training_loss 5.617767333984375 validation_loss 102.92346954345703\n",
      "epoch 17 training_loss 5.2552995681762695 validation_loss 102.95217895507812\n",
      "epoch 17 training_loss 4.978710651397705 validation_loss 102.93225860595703\n",
      "epoch 17 training_loss 5.44652795791626 validation_loss 102.9283447265625\n",
      "epoch 17 training_loss 5.001316547393799 validation_loss 102.95649719238281\n",
      "epoch 17 training_loss 5.19588565826416 validation_loss 102.9402847290039\n",
      "epoch 17 training_loss 5.110970497131348 validation_loss 102.9537124633789\n",
      "epoch 17 training_loss 4.989741325378418 validation_loss 102.97079467773438\n",
      "epoch 17 training_loss 5.33468770980835 validation_loss 103.01101684570312\n",
      "epoch 17 training_loss 5.317658424377441 validation_loss 102.9778823852539\n",
      "epoch 17 training_loss 4.652296543121338 validation_loss 103.01274871826172\n",
      "epoch 17 training_loss 4.923405170440674 validation_loss 102.96910095214844\n",
      "epoch 17 training_loss 5.241566181182861 validation_loss 102.93694305419922\n",
      "epoch 17 training_loss 4.8737311363220215 validation_loss 102.90904235839844\n",
      "epoch 17 training_loss 5.319195747375488 validation_loss 102.90602111816406\n",
      "epoch 17 training_loss 4.823400497436523 validation_loss 102.95513153076172\n",
      "epoch 17 training_loss 5.249814510345459 validation_loss 102.89718627929688\n",
      "epoch 18 training_loss 5.009920120239258 validation_loss 102.91645050048828\n",
      "epoch 18 training_loss 4.878432750701904 validation_loss 102.9854507446289\n",
      "epoch 18 training_loss 5.089195728302002 validation_loss 102.98821258544922\n",
      "epoch 18 training_loss 5.012277126312256 validation_loss 102.93685913085938\n",
      "epoch 18 training_loss 4.912884712219238 validation_loss 102.92047882080078\n",
      "epoch 18 training_loss 5.133161544799805 validation_loss 102.98606872558594\n",
      "epoch 18 training_loss 5.060096263885498 validation_loss 102.93999481201172\n",
      "epoch 18 training_loss 5.121439456939697 validation_loss 102.96397399902344\n",
      "epoch 18 training_loss 5.09163761138916 validation_loss 102.9748764038086\n",
      "epoch 18 training_loss 5.1772308349609375 validation_loss 102.90322875976562\n",
      "epoch 18 training_loss 5.225924491882324 validation_loss 102.90894317626953\n",
      "epoch 18 training_loss 5.248649597167969 validation_loss 102.94419860839844\n",
      "epoch 18 training_loss 4.936121940612793 validation_loss 102.93998718261719\n",
      "epoch 18 training_loss 5.023392200469971 validation_loss 102.9499740600586\n",
      "epoch 18 training_loss 5.607528209686279 validation_loss 102.90415954589844\n",
      "epoch 18 training_loss 5.0516581535339355 validation_loss 102.90745544433594\n",
      "epoch 19 training_loss 5.2629618644714355 validation_loss 102.87210845947266\n",
      "epoch 19 training_loss 4.907026290893555 validation_loss 102.85564422607422\n",
      "epoch 19 training_loss 4.921110153198242 validation_loss 102.88995361328125\n",
      "epoch 19 training_loss 4.887381553649902 validation_loss 102.95979309082031\n",
      "epoch 19 training_loss 4.744442939758301 validation_loss 102.97799682617188\n",
      "epoch 19 training_loss 5.513591766357422 validation_loss 102.99365997314453\n",
      "epoch 19 training_loss 5.2943644523620605 validation_loss 102.99445343017578\n",
      "epoch 19 training_loss 5.212163925170898 validation_loss 102.9249496459961\n",
      "epoch 19 training_loss 5.301033973693848 validation_loss 102.90419006347656\n",
      "epoch 19 training_loss 5.081304550170898 validation_loss 102.96329498291016\n",
      "epoch 19 training_loss 5.1185479164123535 validation_loss 102.94392395019531\n",
      "epoch 19 training_loss 4.847863674163818 validation_loss 102.93665313720703\n",
      "epoch 19 training_loss 5.166233539581299 validation_loss 102.91764068603516\n",
      "epoch 19 training_loss 5.370385646820068 validation_loss 102.91453552246094\n",
      "epoch 19 training_loss 4.978147029876709 validation_loss 102.9298095703125\n",
      "epoch 19 training_loss 4.809707164764404 validation_loss 103.04524230957031\n",
      "epoch 20 training_loss 5.2189154624938965 validation_loss 103.072998046875\n",
      "epoch 20 training_loss 4.761300563812256 validation_loss 102.9672622680664\n",
      "epoch 20 training_loss 5.284740924835205 validation_loss 102.96473693847656\n",
      "epoch 20 training_loss 5.384482383728027 validation_loss 102.8736572265625\n",
      "epoch 20 training_loss 5.182149410247803 validation_loss 102.89728546142578\n",
      "epoch 20 training_loss 5.196841716766357 validation_loss 102.91143798828125\n",
      "epoch 20 training_loss 5.155055522918701 validation_loss 102.96436309814453\n",
      "epoch 20 training_loss 5.023908615112305 validation_loss 102.91024780273438\n",
      "epoch 20 training_loss 5.274575233459473 validation_loss 102.93238067626953\n",
      "epoch 20 training_loss 4.6230573654174805 validation_loss 102.93848419189453\n",
      "epoch 20 training_loss 5.100598335266113 validation_loss 102.9393539428711\n",
      "epoch 20 training_loss 4.822713375091553 validation_loss 102.99542236328125\n",
      "epoch 20 training_loss 5.0616230964660645 validation_loss 102.9697036743164\n",
      "epoch 20 training_loss 5.268918037414551 validation_loss 102.95320129394531\n",
      "epoch 20 training_loss 4.864025592803955 validation_loss 102.923828125\n",
      "epoch 20 training_loss 5.722975254058838 validation_loss 102.84893035888672\n",
      "epoch 21 training_loss 4.999508380889893 validation_loss 102.88846588134766\n",
      "epoch 21 training_loss 5.046839237213135 validation_loss 102.86981201171875\n",
      "epoch 21 training_loss 5.04215145111084 validation_loss 102.91548156738281\n",
      "epoch 21 training_loss 5.204779624938965 validation_loss 102.90635681152344\n",
      "epoch 21 training_loss 5.4522600173950195 validation_loss 102.94514465332031\n",
      "epoch 21 training_loss 5.147130489349365 validation_loss 102.9327163696289\n",
      "epoch 21 training_loss 5.15408992767334 validation_loss 102.9115219116211\n",
      "epoch 21 training_loss 5.038124084472656 validation_loss 102.9135971069336\n",
      "epoch 21 training_loss 4.960113048553467 validation_loss 102.92620086669922\n",
      "epoch 21 training_loss 4.681286334991455 validation_loss 102.9305191040039\n",
      "epoch 21 training_loss 5.0360822677612305 validation_loss 102.96590423583984\n",
      "epoch 21 training_loss 5.090218544006348 validation_loss 102.8745346069336\n",
      "epoch 21 training_loss 4.8698344230651855 validation_loss 102.9244613647461\n",
      "epoch 21 training_loss 5.5986456871032715 validation_loss 102.87184143066406\n",
      "epoch 21 training_loss 5.053165912628174 validation_loss 102.8739242553711\n",
      "epoch 21 training_loss 5.301108360290527 validation_loss 102.91871643066406\n",
      "epoch 22 training_loss 5.494871616363525 validation_loss 102.84676361083984\n",
      "epoch 22 training_loss 4.754574298858643 validation_loss 102.8948974609375\n",
      "epoch 22 training_loss 5.401824474334717 validation_loss 102.90532684326172\n",
      "epoch 22 training_loss 5.036352634429932 validation_loss 102.89359283447266\n",
      "epoch 22 training_loss 5.201662063598633 validation_loss 102.90372467041016\n",
      "epoch 22 training_loss 5.1510396003723145 validation_loss 102.87919616699219\n",
      "epoch 22 training_loss 4.805581569671631 validation_loss 102.93092346191406\n",
      "epoch 22 training_loss 4.818289279937744 validation_loss 102.97572326660156\n",
      "epoch 22 training_loss 5.352831840515137 validation_loss 102.95376586914062\n",
      "epoch 22 training_loss 5.206881999969482 validation_loss 102.95580291748047\n",
      "epoch 22 training_loss 4.881960868835449 validation_loss 102.9083251953125\n",
      "epoch 22 training_loss 5.15043830871582 validation_loss 102.95053100585938\n",
      "epoch 22 training_loss 4.949550151824951 validation_loss 102.90970611572266\n",
      "epoch 22 training_loss 5.342719078063965 validation_loss 102.88524627685547\n",
      "epoch 22 training_loss 4.942385673522949 validation_loss 102.88081359863281\n",
      "epoch 22 training_loss 4.970025062561035 validation_loss 102.93097686767578\n",
      "epoch 23 training_loss 4.615746974945068 validation_loss 102.91386413574219\n",
      "epoch 23 training_loss 5.056333065032959 validation_loss 102.91740417480469\n",
      "epoch 23 training_loss 5.050590991973877 validation_loss 102.94606018066406\n",
      "epoch 23 training_loss 5.152770519256592 validation_loss 102.958251953125\n",
      "epoch 23 training_loss 5.105071067810059 validation_loss 102.97213745117188\n",
      "epoch 23 training_loss 5.43690824508667 validation_loss 102.97430419921875\n",
      "epoch 23 training_loss 5.304438591003418 validation_loss 102.91148376464844\n",
      "epoch 23 training_loss 4.944159984588623 validation_loss 102.8830795288086\n",
      "epoch 23 training_loss 4.808676719665527 validation_loss 102.85435485839844\n",
      "epoch 23 training_loss 4.943630218505859 validation_loss 102.83575439453125\n",
      "epoch 23 training_loss 5.521099090576172 validation_loss 102.84221649169922\n",
      "epoch 23 training_loss 5.324388027191162 validation_loss 102.86849212646484\n",
      "epoch 23 training_loss 5.084343910217285 validation_loss 102.8610610961914\n",
      "epoch 23 training_loss 4.7076826095581055 validation_loss 102.87258911132812\n",
      "epoch 23 training_loss 5.363833904266357 validation_loss 102.88340759277344\n",
      "epoch 23 training_loss 5.097157001495361 validation_loss 102.95455169677734\n",
      "epoch 24 training_loss 5.289013385772705 validation_loss 102.93669891357422\n",
      "epoch 24 training_loss 5.045864105224609 validation_loss 102.9317398071289\n",
      "epoch 24 training_loss 5.316158294677734 validation_loss 102.94197845458984\n",
      "epoch 24 training_loss 5.140304088592529 validation_loss 102.88826751708984\n",
      "epoch 24 training_loss 5.261227130889893 validation_loss 102.83537292480469\n",
      "epoch 24 training_loss 5.181999683380127 validation_loss 102.85918426513672\n",
      "epoch 24 training_loss 4.885932445526123 validation_loss 102.87919616699219\n",
      "epoch 24 training_loss 4.6841349601745605 validation_loss 102.94155883789062\n",
      "epoch 24 training_loss 4.80980110168457 validation_loss 102.94036865234375\n",
      "epoch 24 training_loss 5.087662696838379 validation_loss 102.95301818847656\n",
      "epoch 24 training_loss 5.349519729614258 validation_loss 102.92327880859375\n",
      "epoch 24 training_loss 5.090883731842041 validation_loss 102.89839172363281\n",
      "epoch 24 training_loss 5.203978061676025 validation_loss 102.86396789550781\n",
      "epoch 24 training_loss 5.045156955718994 validation_loss 102.90311431884766\n",
      "epoch 24 training_loss 5.071174621582031 validation_loss 102.88092041015625\n",
      "epoch 24 training_loss 4.9533867835998535 validation_loss 102.79743194580078\n",
      "epoch 25 training_loss 4.791846752166748 validation_loss 102.84164428710938\n",
      "epoch 25 training_loss 4.9467267990112305 validation_loss 102.84178161621094\n",
      "epoch 25 training_loss 5.293399333953857 validation_loss 102.85398864746094\n",
      "epoch 25 training_loss 5.0302581787109375 validation_loss 102.89060974121094\n",
      "epoch 25 training_loss 4.901856422424316 validation_loss 102.90937042236328\n",
      "epoch 25 training_loss 5.275043964385986 validation_loss 102.89201354980469\n",
      "epoch 25 training_loss 4.976208209991455 validation_loss 102.91329956054688\n",
      "epoch 25 training_loss 5.084784984588623 validation_loss 102.88380432128906\n",
      "epoch 25 training_loss 5.30714750289917 validation_loss 102.92198181152344\n",
      "epoch 25 training_loss 5.181552886962891 validation_loss 102.94221496582031\n",
      "epoch 25 training_loss 5.161256313323975 validation_loss 102.92061614990234\n",
      "epoch 25 training_loss 4.991750717163086 validation_loss 102.83575439453125\n",
      "epoch 25 training_loss 5.235344409942627 validation_loss 102.84996795654297\n",
      "epoch 25 training_loss 5.030022621154785 validation_loss 102.85972595214844\n",
      "epoch 25 training_loss 4.925745964050293 validation_loss 102.88117980957031\n",
      "epoch 25 training_loss 5.731353282928467 validation_loss 102.90839385986328\n",
      "epoch 26 training_loss 5.083032608032227 validation_loss 102.84725189208984\n",
      "epoch 26 training_loss 5.434894561767578 validation_loss 102.83474731445312\n",
      "epoch 26 training_loss 4.969113349914551 validation_loss 102.83587646484375\n",
      "epoch 26 training_loss 5.039137363433838 validation_loss 102.89508056640625\n",
      "epoch 26 training_loss 5.1557416915893555 validation_loss 102.88814544677734\n",
      "epoch 26 training_loss 5.176903247833252 validation_loss 102.90420532226562\n",
      "epoch 26 training_loss 5.195594787597656 validation_loss 102.85922241210938\n",
      "epoch 26 training_loss 4.998921871185303 validation_loss 102.88421630859375\n",
      "epoch 26 training_loss 4.872647762298584 validation_loss 102.93122863769531\n",
      "epoch 26 training_loss 4.952823162078857 validation_loss 102.90392303466797\n",
      "epoch 26 training_loss 5.1988115310668945 validation_loss 102.88291931152344\n",
      "epoch 26 training_loss 4.955450057983398 validation_loss 102.90504455566406\n",
      "epoch 26 training_loss 4.88929557800293 validation_loss 102.88431549072266\n",
      "epoch 26 training_loss 5.21422004699707 validation_loss 102.86724853515625\n",
      "epoch 26 training_loss 5.016615390777588 validation_loss 102.89361572265625\n",
      "epoch 26 training_loss 5.637295246124268 validation_loss 102.89096069335938\n",
      "epoch 27 training_loss 4.736473083496094 validation_loss 102.8863754272461\n",
      "epoch 27 training_loss 5.060584545135498 validation_loss 102.87665557861328\n",
      "epoch 27 training_loss 4.807314872741699 validation_loss 102.86959075927734\n",
      "epoch 27 training_loss 5.083683013916016 validation_loss 102.88314056396484\n",
      "epoch 27 training_loss 5.0667877197265625 validation_loss 102.85479736328125\n",
      "epoch 27 training_loss 5.299741268157959 validation_loss 102.90040588378906\n",
      "epoch 27 training_loss 5.081617832183838 validation_loss 102.83775329589844\n",
      "epoch 27 training_loss 5.180408954620361 validation_loss 102.8319091796875\n",
      "epoch 27 training_loss 5.11632776260376 validation_loss 102.85298919677734\n",
      "epoch 27 training_loss 4.882932662963867 validation_loss 102.870361328125\n",
      "epoch 27 training_loss 5.229323863983154 validation_loss 102.90384674072266\n",
      "epoch 27 training_loss 5.347322463989258 validation_loss 102.88068389892578\n",
      "epoch 27 training_loss 4.921907424926758 validation_loss 102.93150329589844\n",
      "epoch 27 training_loss 5.346968650817871 validation_loss 102.92584228515625\n",
      "epoch 27 training_loss 5.26126766204834 validation_loss 102.89912414550781\n",
      "epoch 27 training_loss 4.931363105773926 validation_loss 102.94700622558594\n",
      "epoch 28 training_loss 5.116571426391602 validation_loss 102.93736267089844\n",
      "epoch 28 training_loss 4.919844627380371 validation_loss 102.87989807128906\n",
      "epoch 28 training_loss 4.974395275115967 validation_loss 102.8820571899414\n",
      "epoch 28 training_loss 4.887621879577637 validation_loss 102.85783386230469\n",
      "epoch 28 training_loss 5.24758768081665 validation_loss 102.8139419555664\n",
      "epoch 28 training_loss 4.801324367523193 validation_loss 102.86135864257812\n",
      "epoch 28 training_loss 5.239060401916504 validation_loss 102.84996795654297\n",
      "epoch 28 training_loss 5.2197160720825195 validation_loss 102.91542053222656\n",
      "epoch 28 training_loss 5.0653157234191895 validation_loss 102.90292358398438\n",
      "epoch 28 training_loss 4.853306293487549 validation_loss 102.88835906982422\n",
      "epoch 28 training_loss 5.201986789703369 validation_loss 102.87571716308594\n",
      "epoch 28 training_loss 5.343798637390137 validation_loss 102.87422943115234\n",
      "epoch 28 training_loss 5.138365268707275 validation_loss 102.88015747070312\n",
      "epoch 28 training_loss 4.891299724578857 validation_loss 102.9031753540039\n",
      "epoch 28 training_loss 5.227146625518799 validation_loss 102.82783508300781\n",
      "epoch 28 training_loss 5.624203681945801 validation_loss 102.82867431640625\n",
      "epoch 29 training_loss 5.091195583343506 validation_loss 102.85779571533203\n",
      "epoch 29 training_loss 5.428159236907959 validation_loss 102.80047607421875\n",
      "epoch 29 training_loss 4.948240756988525 validation_loss 102.81505584716797\n",
      "epoch 29 training_loss 5.21779203414917 validation_loss 102.86119079589844\n",
      "epoch 29 training_loss 4.80804967880249 validation_loss 102.91648864746094\n",
      "epoch 29 training_loss 4.748287677764893 validation_loss 102.8926010131836\n",
      "epoch 29 training_loss 5.060945987701416 validation_loss 102.837646484375\n",
      "epoch 29 training_loss 5.366146087646484 validation_loss 102.85518646240234\n",
      "epoch 29 training_loss 5.128361225128174 validation_loss 102.88009643554688\n",
      "epoch 29 training_loss 4.994228839874268 validation_loss 102.85755920410156\n",
      "epoch 29 training_loss 5.179291248321533 validation_loss 102.90193939208984\n",
      "epoch 29 training_loss 4.901636123657227 validation_loss 102.87054443359375\n",
      "epoch 29 training_loss 4.526959419250488 validation_loss 102.90426635742188\n",
      "epoch 29 training_loss 5.480022430419922 validation_loss 102.87169647216797\n",
      "epoch 29 training_loss 5.389191150665283 validation_loss 102.87067413330078\n",
      "epoch 29 training_loss 5.23691987991333 validation_loss 102.83379364013672\n",
      "epoch 30 training_loss 4.952306270599365 validation_loss 102.8314437866211\n",
      "epoch 30 training_loss 5.226009368896484 validation_loss 102.85831451416016\n",
      "epoch 30 training_loss 4.860507011413574 validation_loss 102.87715148925781\n",
      "epoch 30 training_loss 4.994099140167236 validation_loss 102.87661743164062\n",
      "epoch 30 training_loss 4.972591876983643 validation_loss 102.8127670288086\n",
      "epoch 30 training_loss 5.234403133392334 validation_loss 102.84221649169922\n",
      "epoch 30 training_loss 5.241596698760986 validation_loss 102.86956024169922\n",
      "epoch 30 training_loss 5.073441505432129 validation_loss 102.81932830810547\n",
      "epoch 30 training_loss 5.072534084320068 validation_loss 102.81840515136719\n",
      "epoch 30 training_loss 5.094836711883545 validation_loss 102.8802490234375\n",
      "epoch 30 training_loss 4.833685874938965 validation_loss 102.89215850830078\n",
      "epoch 30 training_loss 5.504570960998535 validation_loss 102.85336303710938\n",
      "epoch 30 training_loss 4.922886848449707 validation_loss 102.84281921386719\n",
      "epoch 30 training_loss 5.109710216522217 validation_loss 102.8812484741211\n",
      "epoch 30 training_loss 5.056283473968506 validation_loss 102.88578796386719\n",
      "epoch 30 training_loss 5.491842269897461 validation_loss 102.8818588256836\n",
      "epoch 31 training_loss 5.285776138305664 validation_loss 102.87206268310547\n",
      "epoch 31 training_loss 4.827653408050537 validation_loss 102.92381286621094\n",
      "epoch 31 training_loss 4.941763401031494 validation_loss 102.92980194091797\n",
      "epoch 31 training_loss 5.068833351135254 validation_loss 102.90444946289062\n",
      "epoch 31 training_loss 5.233692169189453 validation_loss 102.92111206054688\n",
      "epoch 31 training_loss 5.033206939697266 validation_loss 102.88677215576172\n",
      "epoch 31 training_loss 4.999680042266846 validation_loss 102.90884399414062\n",
      "epoch 31 training_loss 4.869770526885986 validation_loss 102.89081573486328\n",
      "epoch 31 training_loss 4.977990627288818 validation_loss 102.90695190429688\n",
      "epoch 31 training_loss 5.210999965667725 validation_loss 102.82500457763672\n",
      "epoch 31 training_loss 5.1406049728393555 validation_loss 102.83292388916016\n",
      "epoch 31 training_loss 5.166742324829102 validation_loss 102.81581115722656\n",
      "epoch 31 training_loss 5.419825077056885 validation_loss 102.82968139648438\n",
      "epoch 31 training_loss 4.974104404449463 validation_loss 102.8292007446289\n",
      "epoch 31 training_loss 5.061773300170898 validation_loss 102.8133316040039\n",
      "epoch 31 training_loss 5.294259548187256 validation_loss 102.77629089355469\n",
      "epoch 32 training_loss 5.337301254272461 validation_loss 102.85637664794922\n",
      "epoch 32 training_loss 5.0625433921813965 validation_loss 102.81526184082031\n",
      "epoch 32 training_loss 5.162015438079834 validation_loss 102.82530975341797\n",
      "epoch 32 training_loss 5.35477876663208 validation_loss 102.79432678222656\n",
      "epoch 32 training_loss 5.0832719802856445 validation_loss 102.8603515625\n",
      "epoch 32 training_loss 5.436314582824707 validation_loss 102.89472198486328\n",
      "epoch 32 training_loss 4.93251371383667 validation_loss 102.82840728759766\n",
      "epoch 32 training_loss 5.204164981842041 validation_loss 102.84978485107422\n",
      "epoch 32 training_loss 5.184798717498779 validation_loss 102.87548065185547\n",
      "epoch 32 training_loss 4.720946311950684 validation_loss 102.89585876464844\n",
      "epoch 32 training_loss 5.043973445892334 validation_loss 102.85232543945312\n",
      "epoch 32 training_loss 4.782034397125244 validation_loss 102.85717010498047\n",
      "epoch 32 training_loss 4.870481491088867 validation_loss 102.82003021240234\n",
      "epoch 32 training_loss 4.816373348236084 validation_loss 102.8512191772461\n",
      "epoch 32 training_loss 5.212328910827637 validation_loss 102.84798431396484\n",
      "epoch 32 training_loss 5.290220737457275 validation_loss 102.80189514160156\n",
      "epoch 33 training_loss 5.198526382446289 validation_loss 102.8084487915039\n",
      "epoch 33 training_loss 5.06186056137085 validation_loss 102.75672912597656\n",
      "epoch 33 training_loss 5.001713752746582 validation_loss 102.85694885253906\n",
      "epoch 33 training_loss 5.115019798278809 validation_loss 102.8626937866211\n",
      "epoch 33 training_loss 5.083935737609863 validation_loss 102.87171173095703\n",
      "epoch 33 training_loss 4.848022937774658 validation_loss 102.91187286376953\n",
      "epoch 33 training_loss 4.951807975769043 validation_loss 102.8970947265625\n",
      "epoch 33 training_loss 4.90438985824585 validation_loss 102.90147399902344\n",
      "epoch 33 training_loss 5.177003383636475 validation_loss 102.82627868652344\n",
      "epoch 33 training_loss 5.2834954261779785 validation_loss 102.77693939208984\n",
      "epoch 33 training_loss 5.079622268676758 validation_loss 102.8282241821289\n",
      "epoch 33 training_loss 5.251953125 validation_loss 102.86912536621094\n",
      "epoch 33 training_loss 5.058512210845947 validation_loss 102.85540771484375\n",
      "epoch 33 training_loss 5.209438800811768 validation_loss 102.8263931274414\n",
      "epoch 33 training_loss 5.093391418457031 validation_loss 102.8300552368164\n",
      "epoch 33 training_loss 4.96780252456665 validation_loss 102.89106750488281\n",
      "epoch 34 training_loss 5.229124069213867 validation_loss 102.85665893554688\n",
      "epoch 34 training_loss 4.891513347625732 validation_loss 102.89360809326172\n",
      "epoch 34 training_loss 5.099466800689697 validation_loss 102.85773468017578\n",
      "epoch 34 training_loss 5.276538372039795 validation_loss 102.83920288085938\n",
      "epoch 34 training_loss 5.564600467681885 validation_loss 102.84146118164062\n",
      "epoch 34 training_loss 5.193305969238281 validation_loss 102.8197021484375\n",
      "epoch 34 training_loss 5.273431301116943 validation_loss 102.8133773803711\n",
      "epoch 34 training_loss 5.000448226928711 validation_loss 102.80189514160156\n",
      "epoch 34 training_loss 4.909599781036377 validation_loss 102.83357238769531\n",
      "epoch 34 training_loss 4.958998203277588 validation_loss 102.85807800292969\n",
      "epoch 34 training_loss 4.974682807922363 validation_loss 102.84452056884766\n",
      "epoch 34 training_loss 4.9389238357543945 validation_loss 102.82696533203125\n",
      "epoch 34 training_loss 4.8689422607421875 validation_loss 102.8781967163086\n",
      "epoch 34 training_loss 5.2084126472473145 validation_loss 102.86078643798828\n",
      "epoch 34 training_loss 4.916845798492432 validation_loss 102.8577651977539\n",
      "epoch 34 training_loss 4.957253456115723 validation_loss 102.96748352050781\n",
      "epoch 35 training_loss 5.343197345733643 validation_loss 102.9924087524414\n",
      "epoch 35 training_loss 4.917968273162842 validation_loss 102.97509002685547\n",
      "epoch 35 training_loss 5.058388710021973 validation_loss 102.92180633544922\n",
      "epoch 35 training_loss 5.409160614013672 validation_loss 102.88325500488281\n",
      "epoch 35 training_loss 5.160918235778809 validation_loss 102.86255645751953\n",
      "epoch 35 training_loss 5.035994052886963 validation_loss 102.77562713623047\n",
      "epoch 35 training_loss 5.182744026184082 validation_loss 102.83134460449219\n",
      "epoch 35 training_loss 4.835138320922852 validation_loss 102.8318099975586\n",
      "epoch 35 training_loss 4.643313884735107 validation_loss 102.81796264648438\n",
      "epoch 35 training_loss 4.846168041229248 validation_loss 102.86561584472656\n",
      "epoch 35 training_loss 5.469472885131836 validation_loss 102.79486846923828\n",
      "epoch 35 training_loss 5.007725715637207 validation_loss 102.82061767578125\n",
      "epoch 35 training_loss 5.250449180603027 validation_loss 102.86302947998047\n",
      "epoch 35 training_loss 4.958434581756592 validation_loss 102.88135528564453\n",
      "epoch 35 training_loss 4.994805812835693 validation_loss 102.84300994873047\n",
      "epoch 35 training_loss 5.399891376495361 validation_loss 102.79803466796875\n",
      "epoch 36 training_loss 5.587952136993408 validation_loss 102.7773666381836\n",
      "epoch 36 training_loss 5.042057991027832 validation_loss 102.7922592163086\n",
      "epoch 36 training_loss 4.9914679527282715 validation_loss 102.79364776611328\n",
      "epoch 36 training_loss 5.253737926483154 validation_loss 102.82592010498047\n",
      "epoch 36 training_loss 5.12178373336792 validation_loss 102.78392028808594\n",
      "epoch 36 training_loss 5.26505708694458 validation_loss 102.73330688476562\n",
      "epoch 36 training_loss 4.797591686248779 validation_loss 102.81825256347656\n",
      "epoch 36 training_loss 5.050995826721191 validation_loss 102.78074645996094\n",
      "epoch 36 training_loss 4.79938268661499 validation_loss 102.79283905029297\n",
      "epoch 36 training_loss 4.9420037269592285 validation_loss 102.86370086669922\n",
      "epoch 36 training_loss 5.105175971984863 validation_loss 102.84941864013672\n",
      "epoch 36 training_loss 5.00200891494751 validation_loss 102.85153198242188\n",
      "epoch 36 training_loss 5.0937628746032715 validation_loss 102.90275573730469\n",
      "epoch 36 training_loss 5.038647174835205 validation_loss 102.85306549072266\n",
      "epoch 36 training_loss 4.941368579864502 validation_loss 102.87684631347656\n",
      "epoch 36 training_loss 5.560385704040527 validation_loss 102.89917755126953\n",
      "epoch 37 training_loss 5.956413269042969 validation_loss 102.86106872558594\n",
      "epoch 37 training_loss 5.237410068511963 validation_loss 102.77133178710938\n",
      "epoch 37 training_loss 5.046614170074463 validation_loss 102.765625\n",
      "epoch 37 training_loss 4.701948165893555 validation_loss 102.79254150390625\n",
      "epoch 37 training_loss 5.10277795791626 validation_loss 102.81710815429688\n",
      "epoch 37 training_loss 4.753661632537842 validation_loss 102.8550033569336\n",
      "epoch 37 training_loss 5.03889274597168 validation_loss 102.84014129638672\n",
      "epoch 37 training_loss 5.110619068145752 validation_loss 102.82644653320312\n",
      "epoch 37 training_loss 5.0670857429504395 validation_loss 102.83366394042969\n",
      "epoch 37 training_loss 5.155284404754639 validation_loss 102.84387969970703\n",
      "epoch 37 training_loss 5.089280605316162 validation_loss 102.8309555053711\n",
      "epoch 37 training_loss 5.246086120605469 validation_loss 102.86690521240234\n",
      "epoch 37 training_loss 4.779559135437012 validation_loss 102.8253402709961\n",
      "epoch 37 training_loss 5.044060707092285 validation_loss 102.87886047363281\n",
      "epoch 37 training_loss 4.849484920501709 validation_loss 102.88591003417969\n",
      "epoch 37 training_loss 5.162485599517822 validation_loss 102.78903198242188\n",
      "epoch 38 training_loss 4.8229851722717285 validation_loss 102.83464813232422\n",
      "epoch 38 training_loss 4.990972995758057 validation_loss 102.84133911132812\n",
      "epoch 38 training_loss 5.131605625152588 validation_loss 102.85555267333984\n",
      "epoch 38 training_loss 4.683300018310547 validation_loss 102.89422607421875\n",
      "epoch 38 training_loss 5.327142238616943 validation_loss 102.82734680175781\n",
      "epoch 38 training_loss 4.883681774139404 validation_loss 102.87274932861328\n",
      "epoch 38 training_loss 5.051427841186523 validation_loss 102.81620025634766\n",
      "epoch 38 training_loss 4.995153903961182 validation_loss 102.78038787841797\n",
      "epoch 38 training_loss 5.526996612548828 validation_loss 102.74029541015625\n",
      "epoch 38 training_loss 5.206603050231934 validation_loss 102.7359390258789\n",
      "epoch 38 training_loss 5.196332931518555 validation_loss 102.76507568359375\n",
      "epoch 38 training_loss 5.008365631103516 validation_loss 102.79399871826172\n",
      "epoch 38 training_loss 5.183047771453857 validation_loss 102.86148071289062\n",
      "epoch 38 training_loss 5.2403645515441895 validation_loss 102.84273529052734\n",
      "epoch 38 training_loss 4.811481952667236 validation_loss 102.86543273925781\n",
      "epoch 38 training_loss 5.420957565307617 validation_loss 102.831298828125\n",
      "epoch 39 training_loss 5.000175952911377 validation_loss 102.80135345458984\n",
      "epoch 39 training_loss 5.0051350593566895 validation_loss 102.77960968017578\n",
      "epoch 39 training_loss 4.769039154052734 validation_loss 102.80066680908203\n",
      "epoch 39 training_loss 5.155512809753418 validation_loss 102.81226348876953\n",
      "epoch 39 training_loss 5.412854194641113 validation_loss 102.75978088378906\n",
      "epoch 39 training_loss 5.070883274078369 validation_loss 102.779052734375\n",
      "epoch 39 training_loss 5.170368671417236 validation_loss 102.8338623046875\n",
      "epoch 39 training_loss 5.258216381072998 validation_loss 102.8242416381836\n",
      "epoch 39 training_loss 4.665241718292236 validation_loss 102.86676025390625\n",
      "epoch 39 training_loss 5.031249046325684 validation_loss 102.88786315917969\n",
      "epoch 39 training_loss 5.082539081573486 validation_loss 102.84061431884766\n",
      "epoch 39 training_loss 4.987197399139404 validation_loss 102.84667205810547\n",
      "epoch 39 training_loss 5.093329906463623 validation_loss 102.79141998291016\n",
      "epoch 39 training_loss 5.193398475646973 validation_loss 102.86028289794922\n",
      "epoch 39 training_loss 5.3010382652282715 validation_loss 102.82495880126953\n",
      "epoch 39 training_loss 5.047342777252197 validation_loss 102.87488555908203\n",
      "epoch 40 training_loss 5.312997817993164 validation_loss 102.79800415039062\n",
      "epoch 40 training_loss 5.037545680999756 validation_loss 102.83182525634766\n",
      "epoch 40 training_loss 5.177652359008789 validation_loss 102.82585906982422\n",
      "epoch 40 training_loss 5.010985851287842 validation_loss 102.82233428955078\n",
      "epoch 40 training_loss 4.971150875091553 validation_loss 102.85377502441406\n",
      "epoch 40 training_loss 5.420522689819336 validation_loss 102.8752212524414\n",
      "epoch 40 training_loss 4.794524192810059 validation_loss 102.87178039550781\n",
      "epoch 40 training_loss 4.8827033042907715 validation_loss 102.8478775024414\n",
      "epoch 40 training_loss 4.908658027648926 validation_loss 102.81275177001953\n",
      "epoch 40 training_loss 5.0583624839782715 validation_loss 102.83515930175781\n",
      "epoch 40 training_loss 4.8931708335876465 validation_loss 102.86277770996094\n",
      "epoch 40 training_loss 5.2016706466674805 validation_loss 102.87337493896484\n",
      "epoch 40 training_loss 5.349507808685303 validation_loss 102.88014221191406\n",
      "epoch 40 training_loss 5.052126407623291 validation_loss 102.80519104003906\n",
      "epoch 40 training_loss 5.083490371704102 validation_loss 102.76264190673828\n",
      "epoch 40 training_loss 5.110121250152588 validation_loss 102.69441986083984\n",
      "epoch 41 training_loss 4.750172138214111 validation_loss 102.74290466308594\n",
      "epoch 41 training_loss 5.001161575317383 validation_loss 102.7547836303711\n",
      "epoch 41 training_loss 5.0083465576171875 validation_loss 102.7967758178711\n",
      "epoch 41 training_loss 5.112452983856201 validation_loss 102.7757339477539\n",
      "epoch 41 training_loss 5.451322078704834 validation_loss 102.72035217285156\n",
      "epoch 41 training_loss 5.182514667510986 validation_loss 102.78601837158203\n",
      "epoch 41 training_loss 4.957711696624756 validation_loss 102.81635284423828\n",
      "epoch 41 training_loss 5.068016529083252 validation_loss 102.82987213134766\n",
      "epoch 41 training_loss 5.246858596801758 validation_loss 102.84282684326172\n",
      "epoch 41 training_loss 4.69749116897583 validation_loss 102.86156463623047\n",
      "epoch 41 training_loss 5.166357040405273 validation_loss 102.82059478759766\n",
      "epoch 41 training_loss 5.295890808105469 validation_loss 102.792236328125\n",
      "epoch 41 training_loss 5.0015106201171875 validation_loss 102.7921142578125\n",
      "epoch 41 training_loss 5.181812763214111 validation_loss 102.77595520019531\n",
      "epoch 41 training_loss 5.014889240264893 validation_loss 102.85580444335938\n",
      "epoch 41 training_loss 5.130432605743408 validation_loss 102.7927017211914\n",
      "epoch 42 training_loss 5.158445358276367 validation_loss 102.76656341552734\n",
      "epoch 42 training_loss 5.047319412231445 validation_loss 102.7879409790039\n",
      "epoch 42 training_loss 5.242155075073242 validation_loss 102.77967071533203\n",
      "epoch 42 training_loss 5.129946231842041 validation_loss 102.83246612548828\n",
      "epoch 42 training_loss 4.835426330566406 validation_loss 102.84829711914062\n",
      "epoch 42 training_loss 4.638486385345459 validation_loss 102.83775329589844\n",
      "epoch 42 training_loss 5.139359951019287 validation_loss 102.83151245117188\n",
      "epoch 42 training_loss 4.994312286376953 validation_loss 102.82962036132812\n",
      "epoch 42 training_loss 5.169066905975342 validation_loss 102.80248260498047\n",
      "epoch 42 training_loss 5.21021842956543 validation_loss 102.84915924072266\n",
      "epoch 42 training_loss 5.019621849060059 validation_loss 102.81675720214844\n",
      "epoch 42 training_loss 5.223284721374512 validation_loss 102.80842590332031\n",
      "epoch 42 training_loss 5.185653209686279 validation_loss 102.85613250732422\n",
      "epoch 42 training_loss 5.337161540985107 validation_loss 102.7845458984375\n",
      "epoch 42 training_loss 4.972970485687256 validation_loss 102.81929779052734\n",
      "epoch 42 training_loss 4.677082538604736 validation_loss 102.83099365234375\n",
      "epoch 43 training_loss 4.945898532867432 validation_loss 102.82193756103516\n",
      "epoch 43 training_loss 4.896787643432617 validation_loss 102.88713073730469\n",
      "epoch 43 training_loss 5.111507892608643 validation_loss 102.86089324951172\n",
      "epoch 43 training_loss 4.917248725891113 validation_loss 102.86167907714844\n",
      "epoch 43 training_loss 5.043849468231201 validation_loss 102.8454818725586\n",
      "epoch 43 training_loss 5.07883882522583 validation_loss 102.8396987915039\n",
      "epoch 43 training_loss 5.3537373542785645 validation_loss 102.8276138305664\n",
      "epoch 43 training_loss 5.3518266677856445 validation_loss 102.8319320678711\n",
      "epoch 43 training_loss 5.280754566192627 validation_loss 102.7718734741211\n",
      "epoch 43 training_loss 5.1511688232421875 validation_loss 102.82671356201172\n",
      "epoch 43 training_loss 4.943167209625244 validation_loss 102.81719970703125\n",
      "epoch 43 training_loss 4.957930088043213 validation_loss 102.7774658203125\n",
      "epoch 43 training_loss 5.081881046295166 validation_loss 102.77835083007812\n",
      "epoch 43 training_loss 4.705380439758301 validation_loss 102.82170867919922\n",
      "epoch 43 training_loss 5.167644500732422 validation_loss 102.75442504882812\n",
      "epoch 43 training_loss 5.420287132263184 validation_loss 102.73174285888672\n",
      "epoch 44 training_loss 4.897505283355713 validation_loss 102.73902893066406\n",
      "epoch 44 training_loss 5.014715671539307 validation_loss 102.74362182617188\n",
      "epoch 44 training_loss 5.248680591583252 validation_loss 102.76737213134766\n",
      "epoch 44 training_loss 5.616863250732422 validation_loss 102.74201202392578\n",
      "epoch 44 training_loss 5.096877574920654 validation_loss 102.7845458984375\n",
      "epoch 44 training_loss 5.434144020080566 validation_loss 102.85336303710938\n",
      "epoch 44 training_loss 4.932791233062744 validation_loss 102.7646255493164\n",
      "epoch 44 training_loss 4.877864837646484 validation_loss 102.78661346435547\n",
      "epoch 44 training_loss 5.0510382652282715 validation_loss 102.82421875\n",
      "epoch 44 training_loss 4.9194769859313965 validation_loss 102.80799102783203\n",
      "epoch 44 training_loss 5.014362335205078 validation_loss 102.83206939697266\n",
      "epoch 44 training_loss 4.762611389160156 validation_loss 102.82362365722656\n",
      "epoch 44 training_loss 5.217024326324463 validation_loss 102.84027862548828\n",
      "epoch 44 training_loss 5.423295021057129 validation_loss 102.79458618164062\n",
      "epoch 44 training_loss 4.786994934082031 validation_loss 102.80290985107422\n",
      "epoch 44 training_loss 4.6334686279296875 validation_loss 102.80309295654297\n",
      "epoch 45 training_loss 4.953527927398682 validation_loss 102.82524108886719\n",
      "epoch 45 training_loss 5.025184631347656 validation_loss 102.82201385498047\n",
      "epoch 45 training_loss 5.1041259765625 validation_loss 102.78272247314453\n",
      "epoch 45 training_loss 5.121466159820557 validation_loss 102.82792663574219\n",
      "epoch 45 training_loss 5.272078037261963 validation_loss 102.78054809570312\n",
      "epoch 45 training_loss 4.9867167472839355 validation_loss 102.84530639648438\n",
      "epoch 45 training_loss 5.376734256744385 validation_loss 102.84033966064453\n",
      "epoch 45 training_loss 5.151350021362305 validation_loss 102.83966827392578\n",
      "epoch 45 training_loss 5.008677959442139 validation_loss 102.79241943359375\n",
      "epoch 45 training_loss 5.289368152618408 validation_loss 102.77931213378906\n",
      "epoch 45 training_loss 4.998349666595459 validation_loss 102.73951721191406\n",
      "epoch 45 training_loss 5.112475872039795 validation_loss 102.77760314941406\n",
      "epoch 45 training_loss 5.093855381011963 validation_loss 102.80267333984375\n",
      "epoch 45 training_loss 4.798275947570801 validation_loss 102.82832336425781\n",
      "epoch 45 training_loss 4.841628074645996 validation_loss 102.81945037841797\n",
      "epoch 45 training_loss 4.993206024169922 validation_loss 102.83546447753906\n",
      "epoch 46 training_loss 5.145414352416992 validation_loss 102.80493927001953\n",
      "epoch 46 training_loss 4.941612243652344 validation_loss 102.81413269042969\n",
      "epoch 46 training_loss 4.995561599731445 validation_loss 102.83840942382812\n",
      "epoch 46 training_loss 5.136048793792725 validation_loss 102.77012634277344\n",
      "epoch 46 training_loss 5.237724781036377 validation_loss 102.76181030273438\n",
      "epoch 46 training_loss 5.261476516723633 validation_loss 102.84347534179688\n",
      "epoch 46 training_loss 5.492978572845459 validation_loss 102.84346008300781\n",
      "epoch 46 training_loss 4.994184494018555 validation_loss 102.83230590820312\n",
      "epoch 46 training_loss 5.059330463409424 validation_loss 102.78475189208984\n",
      "epoch 46 training_loss 4.987460136413574 validation_loss 102.80670166015625\n",
      "epoch 46 training_loss 4.813322067260742 validation_loss 102.8023910522461\n",
      "epoch 46 training_loss 4.5895209312438965 validation_loss 102.83834838867188\n",
      "epoch 46 training_loss 5.424870014190674 validation_loss 102.85167694091797\n",
      "epoch 46 training_loss 4.835227966308594 validation_loss 102.82317352294922\n",
      "epoch 46 training_loss 5.1549787521362305 validation_loss 102.82402038574219\n",
      "epoch 46 training_loss 5.11830472946167 validation_loss 102.78060150146484\n",
      "epoch 47 training_loss 5.204631328582764 validation_loss 102.78208923339844\n",
      "epoch 47 training_loss 5.121064186096191 validation_loss 102.88883972167969\n",
      "epoch 47 training_loss 5.036491394042969 validation_loss 102.94326782226562\n",
      "epoch 47 training_loss 4.76240348815918 validation_loss 102.9405517578125\n",
      "epoch 47 training_loss 4.749878406524658 validation_loss 102.9111099243164\n",
      "epoch 47 training_loss 5.038762092590332 validation_loss 102.85944366455078\n",
      "epoch 47 training_loss 5.413649559020996 validation_loss 102.80047607421875\n",
      "epoch 47 training_loss 5.463257312774658 validation_loss 102.77296447753906\n",
      "epoch 47 training_loss 4.460696220397949 validation_loss 102.78046417236328\n",
      "epoch 47 training_loss 5.016820430755615 validation_loss 102.81919860839844\n",
      "epoch 47 training_loss 5.296119689941406 validation_loss 102.7640609741211\n",
      "epoch 47 training_loss 5.426150321960449 validation_loss 102.7058334350586\n",
      "epoch 47 training_loss 5.341233730316162 validation_loss 102.6876220703125\n",
      "epoch 47 training_loss 5.112374782562256 validation_loss 102.74789428710938\n",
      "epoch 47 training_loss 4.680927276611328 validation_loss 102.77769470214844\n",
      "epoch 47 training_loss 4.9506516456604 validation_loss 102.71464538574219\n",
      "epoch 48 training_loss 5.116095066070557 validation_loss 102.74385070800781\n",
      "epoch 48 training_loss 5.239035129547119 validation_loss 102.75289916992188\n",
      "epoch 48 training_loss 4.447685241699219 validation_loss 102.74687194824219\n",
      "epoch 48 training_loss 4.974183559417725 validation_loss 102.7981948852539\n",
      "epoch 48 training_loss 4.958654880523682 validation_loss 102.7512435913086\n",
      "epoch 48 training_loss 4.837158679962158 validation_loss 102.8089828491211\n",
      "epoch 48 training_loss 5.043729305267334 validation_loss 102.80973815917969\n",
      "epoch 48 training_loss 4.968585014343262 validation_loss 102.83500671386719\n",
      "epoch 48 training_loss 5.078958034515381 validation_loss 102.8170166015625\n",
      "epoch 48 training_loss 5.046605110168457 validation_loss 102.76290130615234\n",
      "epoch 48 training_loss 4.965148448944092 validation_loss 102.77667236328125\n",
      "epoch 48 training_loss 5.313379764556885 validation_loss 102.83317565917969\n",
      "epoch 48 training_loss 5.202343940734863 validation_loss 102.80299377441406\n",
      "epoch 48 training_loss 5.2665557861328125 validation_loss 102.81111145019531\n",
      "epoch 48 training_loss 5.401362419128418 validation_loss 102.79571533203125\n",
      "epoch 48 training_loss 5.572109699249268 validation_loss 102.75576782226562\n",
      "epoch 49 training_loss 5.152704238891602 validation_loss 102.7716293334961\n",
      "epoch 49 training_loss 4.959784984588623 validation_loss 102.77149963378906\n",
      "epoch 49 training_loss 4.842891216278076 validation_loss 102.77155303955078\n",
      "epoch 49 training_loss 5.209872722625732 validation_loss 102.75736999511719\n",
      "epoch 49 training_loss 5.013709545135498 validation_loss 102.73831176757812\n",
      "epoch 49 training_loss 4.981764316558838 validation_loss 102.80158233642578\n",
      "epoch 49 training_loss 5.1263885498046875 validation_loss 102.78529357910156\n",
      "epoch 49 training_loss 5.357585906982422 validation_loss 102.7671890258789\n",
      "epoch 49 training_loss 5.210498332977295 validation_loss 102.82679748535156\n",
      "epoch 49 training_loss 4.914207935333252 validation_loss 102.83203125\n",
      "epoch 49 training_loss 4.7239227294921875 validation_loss 102.82487487792969\n",
      "epoch 49 training_loss 5.186242580413818 validation_loss 102.83783721923828\n",
      "epoch 49 training_loss 5.001980304718018 validation_loss 102.81672668457031\n",
      "epoch 49 training_loss 5.123593807220459 validation_loss 102.7974624633789\n",
      "epoch 49 training_loss 5.1882829666137695 validation_loss 102.81709289550781\n",
      "epoch 49 training_loss 5.204275131225586 validation_loss 102.7782211303711\n",
      "epoch 50 training_loss 5.125461101531982 validation_loss 102.7705307006836\n",
      "epoch 50 training_loss 5.0767083168029785 validation_loss 102.79448699951172\n",
      "epoch 50 training_loss 4.919191360473633 validation_loss 102.82775115966797\n",
      "epoch 50 training_loss 5.260185718536377 validation_loss 102.83963775634766\n",
      "epoch 50 training_loss 5.0045552253723145 validation_loss 102.78948211669922\n",
      "epoch 50 training_loss 4.760929584503174 validation_loss 102.77857971191406\n",
      "epoch 50 training_loss 5.181154251098633 validation_loss 102.71556854248047\n",
      "epoch 50 training_loss 5.256490230560303 validation_loss 102.7541732788086\n",
      "epoch 50 training_loss 5.103185653686523 validation_loss 102.73735046386719\n",
      "epoch 50 training_loss 5.175342559814453 validation_loss 102.75577545166016\n",
      "epoch 50 training_loss 5.26890230178833 validation_loss 102.78057098388672\n",
      "epoch 50 training_loss 5.1447248458862305 validation_loss 102.78227233886719\n",
      "epoch 50 training_loss 5.13064432144165 validation_loss 102.83718872070312\n",
      "epoch 50 training_loss 4.989777088165283 validation_loss 102.82218170166016\n",
      "epoch 50 training_loss 4.847137928009033 validation_loss 102.83184814453125\n",
      "epoch 50 training_loss 4.549793720245361 validation_loss 102.86906433105469\n",
      "epoch 51 training_loss 5.325779438018799 validation_loss 102.78821563720703\n",
      "epoch 51 training_loss 5.205294132232666 validation_loss 102.77754211425781\n",
      "epoch 51 training_loss 5.071406364440918 validation_loss 102.77080535888672\n",
      "epoch 51 training_loss 5.085525989532471 validation_loss 102.82331085205078\n",
      "epoch 51 training_loss 5.197112560272217 validation_loss 102.88070678710938\n",
      "epoch 51 training_loss 5.319458961486816 validation_loss 102.83296203613281\n",
      "epoch 51 training_loss 4.851522445678711 validation_loss 102.82976531982422\n",
      "epoch 51 training_loss 4.6644182205200195 validation_loss 102.7933578491211\n",
      "epoch 51 training_loss 5.090453147888184 validation_loss 102.77084350585938\n",
      "epoch 51 training_loss 4.980547904968262 validation_loss 102.78289794921875\n",
      "epoch 51 training_loss 5.233945846557617 validation_loss 102.7860107421875\n",
      "epoch 51 training_loss 5.007938861846924 validation_loss 102.78466033935547\n",
      "epoch 51 training_loss 5.196690559387207 validation_loss 102.79083251953125\n",
      "epoch 51 training_loss 5.03685188293457 validation_loss 102.75614166259766\n",
      "epoch 51 training_loss 4.86707067489624 validation_loss 102.80796813964844\n",
      "epoch 51 training_loss 4.79648494720459 validation_loss 102.7728271484375\n",
      "epoch 52 training_loss 4.987267971038818 validation_loss 102.76858520507812\n",
      "epoch 52 training_loss 5.36518669128418 validation_loss 102.83674621582031\n",
      "epoch 52 training_loss 4.995686054229736 validation_loss 102.82205963134766\n",
      "epoch 52 training_loss 4.976505756378174 validation_loss 102.84593963623047\n",
      "epoch 52 training_loss 5.238866329193115 validation_loss 102.78223419189453\n",
      "epoch 52 training_loss 5.3649797439575195 validation_loss 102.79734802246094\n",
      "epoch 52 training_loss 4.9423956871032715 validation_loss 102.77013397216797\n",
      "epoch 52 training_loss 5.001911640167236 validation_loss 102.74819946289062\n",
      "epoch 52 training_loss 4.966699123382568 validation_loss 102.71578216552734\n",
      "epoch 52 training_loss 5.1702189445495605 validation_loss 102.73712921142578\n",
      "epoch 52 training_loss 4.808706283569336 validation_loss 102.75110626220703\n",
      "epoch 52 training_loss 5.044947624206543 validation_loss 102.814697265625\n",
      "epoch 52 training_loss 5.130837917327881 validation_loss 102.74701690673828\n",
      "epoch 52 training_loss 4.867246627807617 validation_loss 102.76897430419922\n",
      "epoch 52 training_loss 5.233782768249512 validation_loss 102.77433776855469\n",
      "epoch 52 training_loss 4.859579563140869 validation_loss 102.84180450439453\n",
      "epoch 53 training_loss 5.39456033706665 validation_loss 102.86509704589844\n",
      "epoch 53 training_loss 4.790156364440918 validation_loss 102.85041809082031\n",
      "epoch 53 training_loss 5.4195427894592285 validation_loss 102.80892181396484\n",
      "epoch 53 training_loss 4.779716968536377 validation_loss 102.78189086914062\n",
      "epoch 53 training_loss 5.086668968200684 validation_loss 102.7723159790039\n",
      "epoch 53 training_loss 4.9153876304626465 validation_loss 102.77952575683594\n",
      "epoch 53 training_loss 5.233950138092041 validation_loss 102.81938171386719\n",
      "epoch 53 training_loss 5.003605842590332 validation_loss 102.81212615966797\n",
      "epoch 53 training_loss 5.297799110412598 validation_loss 102.74862670898438\n",
      "epoch 53 training_loss 4.8616414070129395 validation_loss 102.75309753417969\n",
      "epoch 53 training_loss 4.563474655151367 validation_loss 102.79759216308594\n",
      "epoch 53 training_loss 5.037015914916992 validation_loss 102.7511215209961\n",
      "epoch 53 training_loss 4.901299953460693 validation_loss 102.71466827392578\n",
      "epoch 53 training_loss 5.1939849853515625 validation_loss 102.76886749267578\n",
      "epoch 53 training_loss 5.253335952758789 validation_loss 102.78893280029297\n",
      "epoch 53 training_loss 5.722326278686523 validation_loss 102.79071807861328\n",
      "epoch 54 training_loss 4.6808366775512695 validation_loss 102.75907897949219\n",
      "epoch 54 training_loss 5.4267897605896 validation_loss 102.79509735107422\n",
      "epoch 54 training_loss 5.297558307647705 validation_loss 102.85426330566406\n",
      "epoch 54 training_loss 4.979212284088135 validation_loss 102.81953430175781\n",
      "epoch 54 training_loss 4.915649890899658 validation_loss 102.80162048339844\n",
      "epoch 54 training_loss 5.291365623474121 validation_loss 102.77127838134766\n",
      "epoch 54 training_loss 5.295392036437988 validation_loss 102.73898315429688\n",
      "epoch 54 training_loss 5.229389667510986 validation_loss 102.71678924560547\n",
      "epoch 54 training_loss 4.934667110443115 validation_loss 102.70027923583984\n",
      "epoch 54 training_loss 4.762005805969238 validation_loss 102.78914642333984\n",
      "epoch 54 training_loss 4.957275390625 validation_loss 102.8095932006836\n",
      "epoch 54 training_loss 5.027235507965088 validation_loss 102.80394744873047\n",
      "epoch 54 training_loss 5.217989921569824 validation_loss 102.80818939208984\n",
      "epoch 54 training_loss 4.710535049438477 validation_loss 102.82105255126953\n",
      "epoch 54 training_loss 5.331464767456055 validation_loss 102.74490356445312\n",
      "epoch 54 training_loss 4.889038562774658 validation_loss 102.68173217773438\n",
      "epoch 55 training_loss 5.144869804382324 validation_loss 102.69931030273438\n",
      "epoch 55 training_loss 4.737998008728027 validation_loss 102.71495056152344\n",
      "epoch 55 training_loss 5.29018497467041 validation_loss 102.74388122558594\n",
      "epoch 55 training_loss 4.825091361999512 validation_loss 102.78046417236328\n",
      "epoch 55 training_loss 5.376677513122559 validation_loss 102.68965911865234\n",
      "epoch 55 training_loss 4.994633674621582 validation_loss 102.68608093261719\n",
      "epoch 55 training_loss 4.554001808166504 validation_loss 102.69048309326172\n",
      "epoch 55 training_loss 4.973654270172119 validation_loss 102.69004821777344\n",
      "epoch 55 training_loss 4.901891708374023 validation_loss 102.76486206054688\n",
      "epoch 55 training_loss 5.1055378913879395 validation_loss 102.8115463256836\n",
      "epoch 55 training_loss 5.158893585205078 validation_loss 102.80150604248047\n",
      "epoch 55 training_loss 5.486321449279785 validation_loss 102.79792022705078\n",
      "epoch 55 training_loss 4.890695095062256 validation_loss 102.80183410644531\n",
      "epoch 55 training_loss 5.422243595123291 validation_loss 102.80984497070312\n",
      "epoch 55 training_loss 4.972626686096191 validation_loss 102.78998565673828\n",
      "epoch 55 training_loss 5.405853748321533 validation_loss 102.8115234375\n",
      "epoch 56 training_loss 5.018121719360352 validation_loss 102.79940795898438\n",
      "epoch 56 training_loss 5.1341047286987305 validation_loss 102.81391143798828\n",
      "epoch 56 training_loss 4.972986221313477 validation_loss 102.77424621582031\n",
      "epoch 56 training_loss 5.163074970245361 validation_loss 102.75102996826172\n",
      "epoch 56 training_loss 5.286600589752197 validation_loss 102.7221908569336\n",
      "epoch 56 training_loss 5.425933361053467 validation_loss 102.71918487548828\n",
      "epoch 56 training_loss 4.800957679748535 validation_loss 102.72645568847656\n",
      "epoch 56 training_loss 5.057818412780762 validation_loss 102.77617645263672\n",
      "epoch 56 training_loss 5.126256465911865 validation_loss 102.76573181152344\n",
      "epoch 56 training_loss 4.793590068817139 validation_loss 102.80583190917969\n",
      "epoch 56 training_loss 4.922418117523193 validation_loss 102.79582214355469\n",
      "epoch 56 training_loss 5.090811729431152 validation_loss 102.79198455810547\n",
      "epoch 56 training_loss 5.134941577911377 validation_loss 102.83060455322266\n",
      "epoch 56 training_loss 4.87391996383667 validation_loss 102.80402374267578\n",
      "epoch 56 training_loss 5.243888854980469 validation_loss 102.79377746582031\n",
      "epoch 56 training_loss 4.852453231811523 validation_loss 102.71707153320312\n",
      "epoch 57 training_loss 5.099386692047119 validation_loss 102.78517150878906\n",
      "epoch 57 training_loss 5.191590785980225 validation_loss 102.7728042602539\n",
      "epoch 57 training_loss 5.133355617523193 validation_loss 102.76756286621094\n",
      "epoch 57 training_loss 5.055929183959961 validation_loss 102.74530029296875\n",
      "epoch 57 training_loss 4.982298374176025 validation_loss 102.78974914550781\n",
      "epoch 57 training_loss 4.933207988739014 validation_loss 102.73863983154297\n",
      "epoch 57 training_loss 5.3739166259765625 validation_loss 102.78535461425781\n",
      "epoch 57 training_loss 5.2236809730529785 validation_loss 102.73307037353516\n",
      "epoch 57 training_loss 5.105926990509033 validation_loss 102.74451446533203\n",
      "epoch 57 training_loss 5.048732757568359 validation_loss 102.75115966796875\n",
      "epoch 57 training_loss 5.056230068206787 validation_loss 102.71968841552734\n",
      "epoch 57 training_loss 5.141421318054199 validation_loss 102.77430725097656\n",
      "epoch 57 training_loss 5.062802314758301 validation_loss 102.80254364013672\n",
      "epoch 57 training_loss 4.884005069732666 validation_loss 102.77838897705078\n",
      "epoch 57 training_loss 4.6184515953063965 validation_loss 102.77631378173828\n",
      "epoch 57 training_loss 5.1613264083862305 validation_loss 102.76467895507812\n",
      "epoch 58 training_loss 5.073153018951416 validation_loss 102.7628402709961\n",
      "epoch 58 training_loss 5.197299957275391 validation_loss 102.77082061767578\n",
      "epoch 58 training_loss 4.969606876373291 validation_loss 102.75895690917969\n",
      "epoch 58 training_loss 4.980139255523682 validation_loss 102.75582885742188\n",
      "epoch 58 training_loss 4.789193153381348 validation_loss 102.79412078857422\n",
      "epoch 58 training_loss 4.891932010650635 validation_loss 102.78923797607422\n",
      "epoch 58 training_loss 5.423110485076904 validation_loss 102.85879516601562\n",
      "epoch 58 training_loss 4.7966694831848145 validation_loss 102.75096130371094\n",
      "epoch 58 training_loss 5.472085952758789 validation_loss 102.75303649902344\n",
      "epoch 58 training_loss 5.58573055267334 validation_loss 102.76162719726562\n",
      "epoch 58 training_loss 4.551591873168945 validation_loss 102.76402282714844\n",
      "epoch 58 training_loss 5.04884672164917 validation_loss 102.77515411376953\n",
      "epoch 58 training_loss 5.202935695648193 validation_loss 102.73249816894531\n",
      "epoch 58 training_loss 4.859245300292969 validation_loss 102.75598907470703\n",
      "epoch 58 training_loss 5.155955791473389 validation_loss 102.71320343017578\n",
      "epoch 58 training_loss 4.914696216583252 validation_loss 102.80744171142578\n",
      "epoch 59 training_loss 4.980719566345215 validation_loss 102.7927474975586\n",
      "epoch 59 training_loss 5.262160778045654 validation_loss 102.71126556396484\n",
      "epoch 59 training_loss 4.873067378997803 validation_loss 102.78368377685547\n",
      "epoch 59 training_loss 4.98065710067749 validation_loss 102.76799774169922\n",
      "epoch 59 training_loss 5.192981719970703 validation_loss 102.73200225830078\n",
      "epoch 59 training_loss 4.955601692199707 validation_loss 102.72396087646484\n",
      "epoch 59 training_loss 5.024995803833008 validation_loss 102.74605560302734\n",
      "epoch 59 training_loss 5.446951866149902 validation_loss 102.76034545898438\n",
      "epoch 59 training_loss 5.122852325439453 validation_loss 102.79596710205078\n",
      "epoch 59 training_loss 4.6058855056762695 validation_loss 102.82579803466797\n",
      "epoch 59 training_loss 5.364096641540527 validation_loss 102.81254577636719\n",
      "epoch 59 training_loss 5.122726917266846 validation_loss 102.8021011352539\n",
      "epoch 59 training_loss 5.092249870300293 validation_loss 102.76422882080078\n",
      "epoch 59 training_loss 5.101848125457764 validation_loss 102.77506256103516\n",
      "epoch 59 training_loss 4.835533142089844 validation_loss 102.7532730102539\n",
      "epoch 59 training_loss 4.970059871673584 validation_loss 102.8243179321289\n",
      "epoch 60 training_loss 5.204367637634277 validation_loss 102.82037353515625\n",
      "epoch 60 training_loss 5.347626686096191 validation_loss 102.80707550048828\n",
      "epoch 60 training_loss 4.950927734375 validation_loss 102.7717514038086\n",
      "epoch 60 training_loss 4.854867935180664 validation_loss 102.71703338623047\n",
      "epoch 60 training_loss 4.844085216522217 validation_loss 102.7517318725586\n",
      "epoch 60 training_loss 4.788707733154297 validation_loss 102.7929458618164\n",
      "epoch 60 training_loss 4.771385192871094 validation_loss 102.8248291015625\n",
      "epoch 60 training_loss 5.274082183837891 validation_loss 102.74078369140625\n",
      "epoch 60 training_loss 4.9275054931640625 validation_loss 102.75312805175781\n",
      "epoch 60 training_loss 5.31865119934082 validation_loss 102.74180603027344\n",
      "epoch 60 training_loss 5.2647199630737305 validation_loss 102.73078155517578\n",
      "epoch 60 training_loss 5.254909992218018 validation_loss 102.73835754394531\n",
      "epoch 60 training_loss 4.926141262054443 validation_loss 102.7679672241211\n",
      "epoch 60 training_loss 5.148346424102783 validation_loss 102.79883575439453\n",
      "epoch 60 training_loss 5.113836288452148 validation_loss 102.78429412841797\n",
      "epoch 60 training_loss 4.872098445892334 validation_loss 102.79180908203125\n",
      "epoch 61 training_loss 4.835174083709717 validation_loss 102.78609466552734\n",
      "epoch 61 training_loss 4.958345890045166 validation_loss 102.80174255371094\n",
      "epoch 61 training_loss 5.0026116371154785 validation_loss 102.79517364501953\n",
      "epoch 61 training_loss 5.04666805267334 validation_loss 102.78860473632812\n",
      "epoch 61 training_loss 4.817180633544922 validation_loss 102.81554412841797\n",
      "epoch 61 training_loss 4.94533109664917 validation_loss 102.84601593017578\n",
      "epoch 61 training_loss 5.03709077835083 validation_loss 102.80001831054688\n",
      "epoch 61 training_loss 5.219796657562256 validation_loss 102.75566101074219\n",
      "epoch 61 training_loss 4.893614768981934 validation_loss 102.76412200927734\n",
      "epoch 61 training_loss 5.035542964935303 validation_loss 102.75070190429688\n",
      "epoch 61 training_loss 5.337672233581543 validation_loss 102.72425079345703\n",
      "epoch 61 training_loss 4.8190598487854 validation_loss 102.70716857910156\n",
      "epoch 61 training_loss 5.61062479019165 validation_loss 102.7093505859375\n",
      "epoch 61 training_loss 5.206383228302002 validation_loss 102.6835708618164\n",
      "epoch 61 training_loss 5.184835910797119 validation_loss 102.77139282226562\n",
      "epoch 61 training_loss 4.939765453338623 validation_loss 102.7656021118164\n",
      "epoch 62 training_loss 5.577731609344482 validation_loss 102.76441955566406\n",
      "epoch 62 training_loss 4.946002006530762 validation_loss 102.72180938720703\n",
      "epoch 62 training_loss 4.543823719024658 validation_loss 102.71101379394531\n",
      "epoch 62 training_loss 4.6439738273620605 validation_loss 102.71392059326172\n",
      "epoch 62 training_loss 5.081090450286865 validation_loss 102.7335205078125\n",
      "epoch 62 training_loss 5.693161964416504 validation_loss 102.7055435180664\n",
      "epoch 62 training_loss 5.306814670562744 validation_loss 102.70635223388672\n",
      "epoch 62 training_loss 4.899621963500977 validation_loss 102.74276733398438\n",
      "epoch 62 training_loss 5.00140380859375 validation_loss 102.77506256103516\n",
      "epoch 62 training_loss 4.808613300323486 validation_loss 102.78376770019531\n",
      "epoch 62 training_loss 4.934229850769043 validation_loss 102.73648071289062\n",
      "epoch 62 training_loss 5.365021705627441 validation_loss 102.75091552734375\n",
      "epoch 62 training_loss 5.2003936767578125 validation_loss 102.769775390625\n",
      "epoch 62 training_loss 5.046144962310791 validation_loss 102.78717803955078\n",
      "epoch 62 training_loss 5.089767932891846 validation_loss 102.81673431396484\n",
      "epoch 62 training_loss 4.441554546356201 validation_loss 102.90647888183594\n",
      "epoch 63 training_loss 4.987966537475586 validation_loss 102.80487060546875\n",
      "epoch 63 training_loss 4.823134422302246 validation_loss 102.82234191894531\n",
      "epoch 63 training_loss 5.204501628875732 validation_loss 102.78972625732422\n",
      "epoch 63 training_loss 5.501912593841553 validation_loss 102.78400421142578\n",
      "epoch 63 training_loss 4.896172046661377 validation_loss 102.82046508789062\n",
      "epoch 63 training_loss 4.872224807739258 validation_loss 102.84444427490234\n",
      "epoch 63 training_loss 4.907480239868164 validation_loss 102.80857849121094\n",
      "epoch 63 training_loss 4.984681606292725 validation_loss 102.74960327148438\n",
      "epoch 63 training_loss 5.021120548248291 validation_loss 102.72985076904297\n",
      "epoch 63 training_loss 4.986701011657715 validation_loss 102.78700256347656\n",
      "epoch 63 training_loss 5.195141315460205 validation_loss 102.81959533691406\n",
      "epoch 63 training_loss 5.085830211639404 validation_loss 102.74787139892578\n",
      "epoch 63 training_loss 5.039029121398926 validation_loss 102.78294372558594\n",
      "epoch 63 training_loss 5.155304908752441 validation_loss 102.76702880859375\n",
      "epoch 63 training_loss 5.070267677307129 validation_loss 102.73607635498047\n",
      "epoch 63 training_loss 5.42811393737793 validation_loss 102.67851257324219\n",
      "epoch 64 training_loss 5.160354137420654 validation_loss 102.67585754394531\n",
      "epoch 64 training_loss 4.898063659667969 validation_loss 102.75800323486328\n",
      "epoch 64 training_loss 4.942382335662842 validation_loss 102.76168060302734\n",
      "epoch 64 training_loss 5.150646209716797 validation_loss 102.7905502319336\n",
      "epoch 64 training_loss 4.967705726623535 validation_loss 102.81198120117188\n",
      "epoch 64 training_loss 5.507162094116211 validation_loss 102.76174926757812\n",
      "epoch 64 training_loss 4.845315456390381 validation_loss 102.72301483154297\n",
      "epoch 64 training_loss 5.127335071563721 validation_loss 102.74089050292969\n",
      "epoch 64 training_loss 4.988164901733398 validation_loss 102.73661804199219\n",
      "epoch 64 training_loss 4.797324180603027 validation_loss 102.6903076171875\n",
      "epoch 64 training_loss 4.969751358032227 validation_loss 102.74822235107422\n",
      "epoch 64 training_loss 5.243145942687988 validation_loss 102.75375366210938\n",
      "epoch 64 training_loss 4.842494964599609 validation_loss 102.75846862792969\n",
      "epoch 64 training_loss 5.461406230926514 validation_loss 102.74313354492188\n",
      "epoch 64 training_loss 5.040863990783691 validation_loss 102.71514892578125\n",
      "epoch 64 training_loss 4.874238014221191 validation_loss 102.73143768310547\n",
      "epoch 65 training_loss 4.5920562744140625 validation_loss 102.72775268554688\n",
      "epoch 65 training_loss 4.73320198059082 validation_loss 102.77139282226562\n",
      "epoch 65 training_loss 5.363140106201172 validation_loss 102.78546142578125\n",
      "epoch 65 training_loss 4.636820316314697 validation_loss 102.84967041015625\n",
      "epoch 65 training_loss 5.108960151672363 validation_loss 102.77400970458984\n",
      "epoch 65 training_loss 5.2114152908325195 validation_loss 102.72856140136719\n",
      "epoch 65 training_loss 5.048360347747803 validation_loss 102.73949432373047\n",
      "epoch 65 training_loss 5.115021705627441 validation_loss 102.74478149414062\n",
      "epoch 65 training_loss 5.159672737121582 validation_loss 102.70629119873047\n",
      "epoch 65 training_loss 4.948548316955566 validation_loss 102.66740417480469\n",
      "epoch 65 training_loss 5.073104381561279 validation_loss 102.67868041992188\n",
      "epoch 65 training_loss 4.927867412567139 validation_loss 102.6926498413086\n",
      "epoch 65 training_loss 5.263930797576904 validation_loss 102.73479461669922\n",
      "epoch 65 training_loss 5.282275199890137 validation_loss 102.76598358154297\n",
      "epoch 65 training_loss 5.292781829833984 validation_loss 102.82795715332031\n",
      "epoch 65 training_loss 5.304848670959473 validation_loss 102.65033721923828\n",
      "epoch 66 training_loss 4.947707176208496 validation_loss 102.66214752197266\n",
      "epoch 66 training_loss 5.123076915740967 validation_loss 102.67401885986328\n",
      "epoch 66 training_loss 5.15917444229126 validation_loss 102.66273498535156\n",
      "epoch 66 training_loss 5.308719158172607 validation_loss 102.65604400634766\n",
      "epoch 66 training_loss 4.595057487487793 validation_loss 102.6976089477539\n",
      "epoch 66 training_loss 5.0127410888671875 validation_loss 102.68553161621094\n",
      "epoch 66 training_loss 5.111542224884033 validation_loss 102.70347595214844\n",
      "epoch 66 training_loss 4.811217308044434 validation_loss 102.72586822509766\n",
      "epoch 66 training_loss 5.526218891143799 validation_loss 102.74454498291016\n",
      "epoch 66 training_loss 4.816770553588867 validation_loss 102.78656005859375\n",
      "epoch 66 training_loss 5.36152982711792 validation_loss 102.73963165283203\n",
      "epoch 66 training_loss 4.9921417236328125 validation_loss 102.74776458740234\n",
      "epoch 66 training_loss 4.7806396484375 validation_loss 102.7602767944336\n",
      "epoch 66 training_loss 5.094973087310791 validation_loss 102.80032348632812\n",
      "epoch 66 training_loss 5.076699256896973 validation_loss 102.7937240600586\n",
      "epoch 66 training_loss 5.366411209106445 validation_loss 102.77102661132812\n",
      "epoch 67 training_loss 4.883971214294434 validation_loss 102.75977325439453\n",
      "epoch 67 training_loss 5.052611351013184 validation_loss 102.71137237548828\n",
      "epoch 67 training_loss 5.065273284912109 validation_loss 102.70680236816406\n",
      "epoch 67 training_loss 4.974207401275635 validation_loss 102.73619079589844\n",
      "epoch 67 training_loss 5.285960674285889 validation_loss 102.73564147949219\n",
      "epoch 67 training_loss 4.9539008140563965 validation_loss 102.74163818359375\n",
      "epoch 67 training_loss 5.515632629394531 validation_loss 102.71803283691406\n",
      "epoch 67 training_loss 4.809070110321045 validation_loss 102.74008178710938\n",
      "epoch 67 training_loss 5.107693672180176 validation_loss 102.7706298828125\n",
      "epoch 67 training_loss 4.9394636154174805 validation_loss 102.81717681884766\n",
      "epoch 67 training_loss 5.349958896636963 validation_loss 102.78856658935547\n",
      "epoch 67 training_loss 4.949460506439209 validation_loss 102.7542953491211\n",
      "epoch 67 training_loss 4.911672592163086 validation_loss 102.76673889160156\n",
      "epoch 67 training_loss 5.255539417266846 validation_loss 102.74321746826172\n",
      "epoch 67 training_loss 4.783126354217529 validation_loss 102.77330780029297\n",
      "epoch 67 training_loss 5.044955730438232 validation_loss 102.76212310791016\n",
      "epoch 68 training_loss 4.968724727630615 validation_loss 102.82205200195312\n",
      "epoch 68 training_loss 4.981678485870361 validation_loss 102.77958679199219\n",
      "epoch 68 training_loss 5.002034664154053 validation_loss 102.78421020507812\n",
      "epoch 68 training_loss 5.039593696594238 validation_loss 102.79450225830078\n",
      "epoch 68 training_loss 4.83908748626709 validation_loss 102.75540161132812\n",
      "epoch 68 training_loss 5.312275409698486 validation_loss 102.74303436279297\n",
      "epoch 68 training_loss 5.238545894622803 validation_loss 102.7529067993164\n",
      "epoch 68 training_loss 5.142375469207764 validation_loss 102.74388122558594\n",
      "epoch 68 training_loss 4.9773359298706055 validation_loss 102.73788452148438\n",
      "epoch 68 training_loss 5.214626789093018 validation_loss 102.72834014892578\n",
      "epoch 68 training_loss 4.97621488571167 validation_loss 102.69450378417969\n",
      "epoch 68 training_loss 5.120791435241699 validation_loss 102.69669342041016\n",
      "epoch 68 training_loss 5.011053562164307 validation_loss 102.74507904052734\n",
      "epoch 68 training_loss 5.0152435302734375 validation_loss 102.76764678955078\n",
      "epoch 68 training_loss 4.861715793609619 validation_loss 102.7363510131836\n",
      "epoch 68 training_loss 5.35291862487793 validation_loss 102.67436981201172\n",
      "epoch 69 training_loss 5.064752101898193 validation_loss 102.66291809082031\n",
      "epoch 69 training_loss 5.056525707244873 validation_loss 102.75531768798828\n",
      "epoch 69 training_loss 4.957577705383301 validation_loss 102.71543884277344\n",
      "epoch 69 training_loss 5.2996368408203125 validation_loss 102.74087524414062\n",
      "epoch 69 training_loss 5.093621730804443 validation_loss 102.68289184570312\n",
      "epoch 69 training_loss 4.892889499664307 validation_loss 102.671142578125\n",
      "epoch 69 training_loss 5.404465675354004 validation_loss 102.6428451538086\n",
      "epoch 69 training_loss 5.061003684997559 validation_loss 102.68717956542969\n",
      "epoch 69 training_loss 5.012101650238037 validation_loss 102.75056457519531\n",
      "epoch 69 training_loss 4.800729274749756 validation_loss 102.7694091796875\n",
      "epoch 69 training_loss 5.276258945465088 validation_loss 102.77113342285156\n",
      "epoch 69 training_loss 4.989776134490967 validation_loss 102.7256088256836\n",
      "epoch 69 training_loss 5.073983192443848 validation_loss 102.71289825439453\n",
      "epoch 69 training_loss 4.941468715667725 validation_loss 102.71717071533203\n",
      "epoch 69 training_loss 4.712070941925049 validation_loss 102.72606658935547\n",
      "epoch 69 training_loss 5.491420745849609 validation_loss 102.75489044189453\n",
      "epoch 70 training_loss 4.928977012634277 validation_loss 102.77188873291016\n",
      "epoch 70 training_loss 5.069453239440918 validation_loss 102.75289154052734\n",
      "epoch 70 training_loss 5.313311576843262 validation_loss 102.78630065917969\n",
      "epoch 70 training_loss 5.273812770843506 validation_loss 102.80532836914062\n",
      "epoch 70 training_loss 5.256042957305908 validation_loss 102.71646118164062\n",
      "epoch 70 training_loss 5.380908966064453 validation_loss 102.67465209960938\n",
      "epoch 70 training_loss 4.545015811920166 validation_loss 102.76760864257812\n",
      "epoch 70 training_loss 5.130694389343262 validation_loss 102.7400131225586\n",
      "epoch 70 training_loss 5.028256416320801 validation_loss 102.76958465576172\n",
      "epoch 70 training_loss 4.626028537750244 validation_loss 102.73877716064453\n",
      "epoch 70 training_loss 5.209063529968262 validation_loss 102.69308471679688\n",
      "epoch 70 training_loss 5.106573581695557 validation_loss 102.74345397949219\n",
      "epoch 70 training_loss 5.037520408630371 validation_loss 102.7804183959961\n",
      "epoch 70 training_loss 5.023849964141846 validation_loss 102.72532653808594\n",
      "epoch 70 training_loss 4.9602179527282715 validation_loss 102.71990203857422\n",
      "epoch 70 training_loss 4.840970039367676 validation_loss 102.6455307006836\n",
      "epoch 71 training_loss 5.150998115539551 validation_loss 102.6518783569336\n",
      "epoch 71 training_loss 5.342730522155762 validation_loss 102.67849731445312\n",
      "epoch 71 training_loss 4.947057723999023 validation_loss 102.71810150146484\n",
      "epoch 71 training_loss 4.93106746673584 validation_loss 102.77627563476562\n",
      "epoch 71 training_loss 4.796425819396973 validation_loss 102.760498046875\n",
      "epoch 71 training_loss 5.138507843017578 validation_loss 102.7607650756836\n",
      "epoch 71 training_loss 5.168079853057861 validation_loss 102.7968521118164\n",
      "epoch 71 training_loss 4.684225559234619 validation_loss 102.80506896972656\n",
      "epoch 71 training_loss 5.319636821746826 validation_loss 102.76447296142578\n",
      "epoch 71 training_loss 5.251747131347656 validation_loss 102.76038360595703\n",
      "epoch 71 training_loss 5.099358558654785 validation_loss 102.7384262084961\n",
      "epoch 71 training_loss 4.9027018547058105 validation_loss 102.76447296142578\n",
      "epoch 71 training_loss 5.220150947570801 validation_loss 102.72743225097656\n",
      "epoch 71 training_loss 4.651286602020264 validation_loss 102.71372985839844\n",
      "epoch 71 training_loss 5.139914035797119 validation_loss 102.66865539550781\n",
      "epoch 71 training_loss 5.16733980178833 validation_loss 102.64387512207031\n",
      "epoch 72 training_loss 5.0552544593811035 validation_loss 102.69532775878906\n",
      "epoch 72 training_loss 5.15037202835083 validation_loss 102.65345001220703\n",
      "epoch 72 training_loss 4.88807487487793 validation_loss 102.65747833251953\n",
      "epoch 72 training_loss 5.089835166931152 validation_loss 102.69064331054688\n",
      "epoch 72 training_loss 5.520183563232422 validation_loss 102.69866180419922\n",
      "epoch 72 training_loss 5.047575950622559 validation_loss 102.68873596191406\n",
      "epoch 72 training_loss 5.0333099365234375 validation_loss 102.72660064697266\n",
      "epoch 72 training_loss 5.090508937835693 validation_loss 102.7334976196289\n",
      "epoch 72 training_loss 4.996693134307861 validation_loss 102.76278686523438\n",
      "epoch 72 training_loss 5.153831481933594 validation_loss 102.73503112792969\n",
      "epoch 72 training_loss 4.917808532714844 validation_loss 102.78160858154297\n",
      "epoch 72 training_loss 5.1837286949157715 validation_loss 102.7397689819336\n",
      "epoch 72 training_loss 4.778562545776367 validation_loss 102.74726867675781\n",
      "epoch 72 training_loss 4.933182716369629 validation_loss 102.72216033935547\n",
      "epoch 72 training_loss 4.877485275268555 validation_loss 102.71795654296875\n",
      "epoch 72 training_loss 5.210895538330078 validation_loss 102.75263977050781\n",
      "epoch 73 training_loss 5.0631818771362305 validation_loss 102.78015899658203\n",
      "epoch 73 training_loss 5.004115104675293 validation_loss 102.72457122802734\n",
      "epoch 73 training_loss 5.333256244659424 validation_loss 102.77928161621094\n",
      "epoch 73 training_loss 4.837088108062744 validation_loss 102.81912231445312\n",
      "epoch 73 training_loss 4.810406684875488 validation_loss 102.79634857177734\n",
      "epoch 73 training_loss 4.87040901184082 validation_loss 102.7734375\n",
      "epoch 73 training_loss 5.136495113372803 validation_loss 102.75218200683594\n",
      "epoch 73 training_loss 5.202365875244141 validation_loss 102.74372863769531\n",
      "epoch 73 training_loss 4.975707054138184 validation_loss 102.78055572509766\n",
      "epoch 73 training_loss 5.346067428588867 validation_loss 102.75556945800781\n",
      "epoch 73 training_loss 5.468636989593506 validation_loss 102.6941146850586\n",
      "epoch 73 training_loss 4.584652900695801 validation_loss 102.72640991210938\n",
      "epoch 73 training_loss 5.144237041473389 validation_loss 102.71766662597656\n",
      "epoch 73 training_loss 5.134736061096191 validation_loss 102.67733764648438\n",
      "epoch 73 training_loss 4.937616348266602 validation_loss 102.71234893798828\n",
      "epoch 73 training_loss 4.853906154632568 validation_loss 102.60226440429688\n",
      "epoch 74 training_loss 5.1468634605407715 validation_loss 102.59874725341797\n",
      "epoch 74 training_loss 5.021974563598633 validation_loss 102.62012481689453\n",
      "epoch 74 training_loss 4.808119297027588 validation_loss 102.62899780273438\n",
      "epoch 74 training_loss 4.9891676902771 validation_loss 102.64633178710938\n",
      "epoch 74 training_loss 5.192152500152588 validation_loss 102.67903137207031\n",
      "epoch 74 training_loss 5.032136917114258 validation_loss 102.69681549072266\n",
      "epoch 74 training_loss 4.664153099060059 validation_loss 102.72809600830078\n",
      "epoch 74 training_loss 5.409514427185059 validation_loss 102.72674560546875\n",
      "epoch 74 training_loss 5.033780097961426 validation_loss 102.71815490722656\n",
      "epoch 74 training_loss 5.049839019775391 validation_loss 102.69513702392578\n",
      "epoch 74 training_loss 5.057876110076904 validation_loss 102.67191314697266\n",
      "epoch 74 training_loss 4.866240978240967 validation_loss 102.69926452636719\n",
      "epoch 74 training_loss 5.190840244293213 validation_loss 102.76762390136719\n",
      "epoch 74 training_loss 5.03546667098999 validation_loss 102.7670669555664\n",
      "epoch 74 training_loss 5.304428577423096 validation_loss 102.7853775024414\n",
      "epoch 74 training_loss 4.941400051116943 validation_loss 102.77537536621094\n",
      "epoch 75 training_loss 5.472074031829834 validation_loss 102.76116180419922\n",
      "epoch 75 training_loss 5.157044410705566 validation_loss 102.70687866210938\n",
      "epoch 75 training_loss 4.8622589111328125 validation_loss 102.74795532226562\n",
      "epoch 75 training_loss 5.068361759185791 validation_loss 102.73251342773438\n",
      "epoch 75 training_loss 5.0958027839660645 validation_loss 102.7314453125\n",
      "epoch 75 training_loss 4.87277889251709 validation_loss 102.78295135498047\n",
      "epoch 75 training_loss 5.1545233726501465 validation_loss 102.80068969726562\n",
      "epoch 75 training_loss 5.026437759399414 validation_loss 102.75140380859375\n",
      "epoch 75 training_loss 4.946256160736084 validation_loss 102.7330322265625\n",
      "epoch 75 training_loss 5.308791160583496 validation_loss 102.78118133544922\n",
      "epoch 75 training_loss 4.882328987121582 validation_loss 102.83345031738281\n",
      "epoch 75 training_loss 4.879563331604004 validation_loss 102.80619812011719\n",
      "epoch 75 training_loss 5.204340934753418 validation_loss 102.75477600097656\n",
      "epoch 75 training_loss 4.852667808532715 validation_loss 102.74505615234375\n",
      "epoch 75 training_loss 5.057351589202881 validation_loss 102.67268371582031\n",
      "epoch 75 training_loss 4.8242363929748535 validation_loss 102.65249633789062\n",
      "epoch 76 training_loss 5.12544059753418 validation_loss 102.65826416015625\n",
      "epoch 76 training_loss 5.160850524902344 validation_loss 102.71060180664062\n",
      "epoch 76 training_loss 4.853332042694092 validation_loss 102.75171661376953\n",
      "epoch 76 training_loss 5.002711296081543 validation_loss 102.72702026367188\n",
      "epoch 76 training_loss 4.853439807891846 validation_loss 102.71903991699219\n",
      "epoch 76 training_loss 5.151310443878174 validation_loss 102.71544647216797\n",
      "epoch 76 training_loss 5.074048042297363 validation_loss 102.6930160522461\n",
      "epoch 76 training_loss 5.188978672027588 validation_loss 102.70175170898438\n",
      "epoch 76 training_loss 5.366704940795898 validation_loss 102.7545166015625\n",
      "epoch 76 training_loss 5.171670436859131 validation_loss 102.80429077148438\n",
      "epoch 76 training_loss 4.985937595367432 validation_loss 102.77312469482422\n",
      "epoch 76 training_loss 5.104763031005859 validation_loss 102.762451171875\n",
      "epoch 76 training_loss 5.068487167358398 validation_loss 102.67840576171875\n",
      "epoch 76 training_loss 4.840396881103516 validation_loss 102.67979431152344\n",
      "epoch 76 training_loss 4.874539375305176 validation_loss 102.65389251708984\n",
      "epoch 76 training_loss 4.842467784881592 validation_loss 102.67948150634766\n",
      "epoch 77 training_loss 5.161445140838623 validation_loss 102.64146423339844\n",
      "epoch 77 training_loss 5.082515239715576 validation_loss 102.70509338378906\n",
      "epoch 77 training_loss 4.883825302124023 validation_loss 102.72822570800781\n",
      "epoch 77 training_loss 5.002635955810547 validation_loss 102.73260498046875\n",
      "epoch 77 training_loss 5.159996032714844 validation_loss 102.72551727294922\n",
      "epoch 77 training_loss 5.002251625061035 validation_loss 102.75301361083984\n",
      "epoch 77 training_loss 4.849159240722656 validation_loss 102.75697326660156\n",
      "epoch 77 training_loss 5.327465057373047 validation_loss 102.75847625732422\n",
      "epoch 77 training_loss 5.09266996383667 validation_loss 102.78915405273438\n",
      "epoch 77 training_loss 5.030938625335693 validation_loss 102.75800323486328\n",
      "epoch 77 training_loss 4.869332790374756 validation_loss 102.73904418945312\n",
      "epoch 77 training_loss 4.851066589355469 validation_loss 102.74169158935547\n",
      "epoch 77 training_loss 5.103506088256836 validation_loss 102.76127624511719\n",
      "epoch 77 training_loss 5.1680779457092285 validation_loss 102.7118911743164\n",
      "epoch 77 training_loss 5.073323726654053 validation_loss 102.69215393066406\n",
      "epoch 77 training_loss 5.218616008758545 validation_loss 102.58662414550781\n",
      "epoch 78 training_loss 5.011752605438232 validation_loss 102.63400268554688\n",
      "epoch 78 training_loss 4.89516544342041 validation_loss 102.70079040527344\n",
      "epoch 78 training_loss 5.187598705291748 validation_loss 102.6192398071289\n",
      "epoch 78 training_loss 4.78830623626709 validation_loss 102.63200378417969\n",
      "epoch 78 training_loss 4.885146141052246 validation_loss 102.66154479980469\n",
      "epoch 78 training_loss 4.695654392242432 validation_loss 102.67381286621094\n",
      "epoch 78 training_loss 5.202978134155273 validation_loss 102.71759033203125\n",
      "epoch 78 training_loss 5.394620895385742 validation_loss 102.73407745361328\n",
      "epoch 78 training_loss 4.982527256011963 validation_loss 102.752197265625\n",
      "epoch 78 training_loss 4.841503620147705 validation_loss 102.79607391357422\n",
      "epoch 78 training_loss 5.037286281585693 validation_loss 102.74866485595703\n",
      "epoch 78 training_loss 5.0259904861450195 validation_loss 102.70612335205078\n",
      "epoch 78 training_loss 5.433495998382568 validation_loss 102.63817596435547\n",
      "epoch 78 training_loss 4.793864727020264 validation_loss 102.67164611816406\n",
      "epoch 78 training_loss 5.302605152130127 validation_loss 102.69586181640625\n",
      "epoch 78 training_loss 5.649069309234619 validation_loss 102.66340637207031\n",
      "epoch 79 training_loss 5.094404697418213 validation_loss 102.67216491699219\n",
      "epoch 79 training_loss 5.174506187438965 validation_loss 102.72899627685547\n",
      "epoch 79 training_loss 4.983410358428955 validation_loss 102.73457336425781\n",
      "epoch 79 training_loss 4.961173057556152 validation_loss 102.70069122314453\n",
      "epoch 79 training_loss 5.270641326904297 validation_loss 102.70357513427734\n",
      "epoch 79 training_loss 4.914971828460693 validation_loss 102.7380599975586\n",
      "epoch 79 training_loss 5.169663429260254 validation_loss 102.7408676147461\n",
      "epoch 79 training_loss 4.995608329772949 validation_loss 102.73345184326172\n",
      "epoch 79 training_loss 4.981574058532715 validation_loss 102.74935150146484\n",
      "epoch 79 training_loss 5.037069797515869 validation_loss 102.71772003173828\n",
      "epoch 79 training_loss 5.0483527183532715 validation_loss 102.70475769042969\n",
      "epoch 79 training_loss 4.815365791320801 validation_loss 102.69837951660156\n",
      "epoch 79 training_loss 4.737964153289795 validation_loss 102.73609924316406\n",
      "epoch 79 training_loss 5.157361030578613 validation_loss 102.74690246582031\n",
      "epoch 79 training_loss 5.117906093597412 validation_loss 102.72129821777344\n",
      "epoch 79 training_loss 5.657913684844971 validation_loss 102.64337921142578\n",
      "epoch 80 training_loss 5.051447868347168 validation_loss 102.6658935546875\n",
      "epoch 80 training_loss 4.695991516113281 validation_loss 102.69783020019531\n",
      "epoch 80 training_loss 5.097092628479004 validation_loss 102.68223571777344\n",
      "epoch 80 training_loss 5.329758167266846 validation_loss 102.7007827758789\n",
      "epoch 80 training_loss 4.763566017150879 validation_loss 102.70246887207031\n",
      "epoch 80 training_loss 4.9109578132629395 validation_loss 102.71124267578125\n",
      "epoch 80 training_loss 5.205219745635986 validation_loss 102.71121978759766\n",
      "epoch 80 training_loss 4.649657726287842 validation_loss 102.75299072265625\n",
      "epoch 80 training_loss 4.872069358825684 validation_loss 102.724609375\n",
      "epoch 80 training_loss 5.138314247131348 validation_loss 102.74663543701172\n",
      "epoch 80 training_loss 5.06735372543335 validation_loss 102.7154312133789\n",
      "epoch 80 training_loss 4.929002285003662 validation_loss 102.6681137084961\n",
      "epoch 80 training_loss 5.244383811950684 validation_loss 102.71546936035156\n",
      "epoch 80 training_loss 5.364614486694336 validation_loss 102.66523742675781\n",
      "epoch 80 training_loss 5.3603057861328125 validation_loss 102.6871566772461\n",
      "epoch 80 training_loss 5.0920209884643555 validation_loss 102.71460723876953\n",
      "epoch 81 training_loss 5.149034023284912 validation_loss 102.74871826171875\n",
      "epoch 81 training_loss 4.888355255126953 validation_loss 102.70497131347656\n",
      "epoch 81 training_loss 5.22037410736084 validation_loss 102.6991195678711\n",
      "epoch 81 training_loss 5.473519802093506 validation_loss 102.65760803222656\n",
      "epoch 81 training_loss 5.013956069946289 validation_loss 102.66200256347656\n",
      "epoch 81 training_loss 5.312341213226318 validation_loss 102.65129089355469\n",
      "epoch 81 training_loss 4.99941349029541 validation_loss 102.68999481201172\n",
      "epoch 81 training_loss 4.620241641998291 validation_loss 102.72474670410156\n",
      "epoch 81 training_loss 4.9080657958984375 validation_loss 102.70133972167969\n",
      "epoch 81 training_loss 4.994143962860107 validation_loss 102.71912384033203\n",
      "epoch 81 training_loss 5.305697441101074 validation_loss 102.73725891113281\n",
      "epoch 81 training_loss 4.926630973815918 validation_loss 102.80561828613281\n",
      "epoch 81 training_loss 4.932780742645264 validation_loss 102.73693084716797\n",
      "epoch 81 training_loss 4.885106086730957 validation_loss 102.76007080078125\n",
      "epoch 81 training_loss 5.132073402404785 validation_loss 102.70952606201172\n",
      "epoch 81 training_loss 4.862061977386475 validation_loss 102.74535369873047\n",
      "epoch 82 training_loss 5.139700412750244 validation_loss 102.76304626464844\n",
      "epoch 82 training_loss 4.713312149047852 validation_loss 102.77133178710938\n",
      "epoch 82 training_loss 4.778369426727295 validation_loss 102.78460693359375\n",
      "epoch 82 training_loss 4.845837116241455 validation_loss 102.73958587646484\n",
      "epoch 82 training_loss 4.981863498687744 validation_loss 102.6917724609375\n",
      "epoch 82 training_loss 5.552165508270264 validation_loss 102.7049331665039\n",
      "epoch 82 training_loss 4.828382968902588 validation_loss 102.7275619506836\n",
      "epoch 82 training_loss 5.091007232666016 validation_loss 102.64718627929688\n",
      "epoch 82 training_loss 5.172177314758301 validation_loss 102.63935852050781\n",
      "epoch 82 training_loss 5.4337286949157715 validation_loss 102.59308624267578\n",
      "epoch 82 training_loss 5.011616230010986 validation_loss 102.66056060791016\n",
      "epoch 82 training_loss 5.007683753967285 validation_loss 102.71542358398438\n",
      "epoch 82 training_loss 4.9646453857421875 validation_loss 102.7602310180664\n",
      "epoch 82 training_loss 5.06724214553833 validation_loss 102.74697875976562\n",
      "epoch 82 training_loss 4.825189590454102 validation_loss 102.73041534423828\n",
      "epoch 82 training_loss 5.700265407562256 validation_loss 102.67697143554688\n",
      "epoch 83 training_loss 5.22380256652832 validation_loss 102.65287017822266\n",
      "epoch 83 training_loss 5.256819248199463 validation_loss 102.67499542236328\n",
      "epoch 83 training_loss 5.139565944671631 validation_loss 102.65691375732422\n",
      "epoch 83 training_loss 5.244236469268799 validation_loss 102.71603393554688\n",
      "epoch 83 training_loss 5.060230255126953 validation_loss 102.6833724975586\n",
      "epoch 83 training_loss 5.146693229675293 validation_loss 102.65760040283203\n",
      "epoch 83 training_loss 5.217517375946045 validation_loss 102.62945556640625\n",
      "epoch 83 training_loss 4.841800689697266 validation_loss 102.66002655029297\n",
      "epoch 83 training_loss 4.6903839111328125 validation_loss 102.7016372680664\n",
      "epoch 83 training_loss 4.690896511077881 validation_loss 102.73184967041016\n",
      "epoch 83 training_loss 5.25323486328125 validation_loss 102.70823669433594\n",
      "epoch 83 training_loss 4.875622272491455 validation_loss 102.7151870727539\n",
      "epoch 83 training_loss 5.161771774291992 validation_loss 102.74069213867188\n",
      "epoch 83 training_loss 4.884126663208008 validation_loss 102.75584411621094\n",
      "epoch 83 training_loss 5.044178485870361 validation_loss 102.67340850830078\n",
      "epoch 83 training_loss 4.888580322265625 validation_loss 102.71966552734375\n",
      "epoch 84 training_loss 5.088412761688232 validation_loss 102.70055389404297\n",
      "epoch 84 training_loss 5.051161289215088 validation_loss 102.67462921142578\n",
      "epoch 84 training_loss 5.189250946044922 validation_loss 102.68010711669922\n",
      "epoch 84 training_loss 4.920547008514404 validation_loss 102.71551513671875\n",
      "epoch 84 training_loss 5.553069591522217 validation_loss 102.67646789550781\n",
      "epoch 84 training_loss 4.908318519592285 validation_loss 102.70079803466797\n",
      "epoch 84 training_loss 5.285521030426025 validation_loss 102.70735931396484\n",
      "epoch 84 training_loss 4.68624210357666 validation_loss 102.71881103515625\n",
      "epoch 84 training_loss 4.730382919311523 validation_loss 102.73382568359375\n",
      "epoch 84 training_loss 5.356629848480225 validation_loss 102.70024871826172\n",
      "epoch 84 training_loss 4.617600440979004 validation_loss 102.70217895507812\n",
      "epoch 84 training_loss 4.915989875793457 validation_loss 102.71601867675781\n",
      "epoch 84 training_loss 4.744418621063232 validation_loss 102.73849487304688\n",
      "epoch 84 training_loss 5.486031532287598 validation_loss 102.72566986083984\n",
      "epoch 84 training_loss 5.0818400382995605 validation_loss 102.71124267578125\n",
      "epoch 84 training_loss 5.14486026763916 validation_loss 102.7125015258789\n",
      "epoch 85 training_loss 4.975563049316406 validation_loss 102.70343017578125\n",
      "epoch 85 training_loss 4.998249530792236 validation_loss 102.70032501220703\n",
      "epoch 85 training_loss 5.245097637176514 validation_loss 102.69831848144531\n",
      "epoch 85 training_loss 4.9378156661987305 validation_loss 102.7583999633789\n",
      "epoch 85 training_loss 4.7535905838012695 validation_loss 102.8108139038086\n",
      "epoch 85 training_loss 4.8252644538879395 validation_loss 102.80772399902344\n",
      "epoch 85 training_loss 5.1147027015686035 validation_loss 102.7580795288086\n",
      "epoch 85 training_loss 4.696662425994873 validation_loss 102.72061920166016\n",
      "epoch 85 training_loss 5.065755367279053 validation_loss 102.68746948242188\n",
      "epoch 85 training_loss 5.133047103881836 validation_loss 102.68791961669922\n",
      "epoch 85 training_loss 5.1204729080200195 validation_loss 102.66390991210938\n",
      "epoch 85 training_loss 5.277685165405273 validation_loss 102.64391326904297\n",
      "epoch 85 training_loss 5.304784297943115 validation_loss 102.6580810546875\n",
      "epoch 85 training_loss 5.15653133392334 validation_loss 102.66903686523438\n",
      "epoch 85 training_loss 4.9054951667785645 validation_loss 102.67588806152344\n",
      "epoch 85 training_loss 5.376865863800049 validation_loss 102.62190246582031\n",
      "epoch 86 training_loss 4.829684257507324 validation_loss 102.62789154052734\n",
      "epoch 86 training_loss 4.6842360496521 validation_loss 102.68125915527344\n",
      "epoch 86 training_loss 4.9407877922058105 validation_loss 102.69812774658203\n",
      "epoch 86 training_loss 5.544585227966309 validation_loss 102.684814453125\n",
      "epoch 86 training_loss 5.018707752227783 validation_loss 102.6769790649414\n",
      "epoch 86 training_loss 5.328705787658691 validation_loss 102.69969177246094\n",
      "epoch 86 training_loss 5.059802532196045 validation_loss 102.68926239013672\n",
      "epoch 86 training_loss 5.088751316070557 validation_loss 102.68367767333984\n",
      "epoch 86 training_loss 4.725879669189453 validation_loss 102.70580291748047\n",
      "epoch 86 training_loss 4.90168571472168 validation_loss 102.74596405029297\n",
      "epoch 86 training_loss 5.0296196937561035 validation_loss 102.7794418334961\n",
      "epoch 86 training_loss 5.033309459686279 validation_loss 102.72787475585938\n",
      "epoch 86 training_loss 5.0954203605651855 validation_loss 102.72550964355469\n",
      "epoch 86 training_loss 5.153787612915039 validation_loss 102.69021606445312\n",
      "epoch 86 training_loss 5.0954790115356445 validation_loss 102.68155670166016\n",
      "epoch 86 training_loss 5.3065972328186035 validation_loss 102.57536315917969\n",
      "epoch 87 training_loss 4.935621738433838 validation_loss 102.55416107177734\n",
      "epoch 87 training_loss 5.044682502746582 validation_loss 102.57720184326172\n",
      "epoch 87 training_loss 5.249260425567627 validation_loss 102.62467956542969\n",
      "epoch 87 training_loss 5.023679733276367 validation_loss 102.63457489013672\n",
      "epoch 87 training_loss 4.880820274353027 validation_loss 102.63105010986328\n",
      "epoch 87 training_loss 4.949301242828369 validation_loss 102.68272399902344\n",
      "epoch 87 training_loss 5.259149551391602 validation_loss 102.63961029052734\n",
      "epoch 87 training_loss 5.131498336791992 validation_loss 102.67311096191406\n",
      "epoch 87 training_loss 5.279898643493652 validation_loss 102.72844696044922\n",
      "epoch 87 training_loss 5.436516761779785 validation_loss 102.70057678222656\n",
      "epoch 87 training_loss 4.959130764007568 validation_loss 102.70397186279297\n",
      "epoch 87 training_loss 4.836923122406006 validation_loss 102.71981048583984\n",
      "epoch 87 training_loss 4.868491172790527 validation_loss 102.74140930175781\n",
      "epoch 87 training_loss 5.1000447273254395 validation_loss 102.73474884033203\n",
      "epoch 87 training_loss 4.717329978942871 validation_loss 102.69588470458984\n",
      "epoch 87 training_loss 4.9351701736450195 validation_loss 102.66410827636719\n",
      "epoch 88 training_loss 4.800572872161865 validation_loss 102.68572235107422\n",
      "epoch 88 training_loss 5.041225433349609 validation_loss 102.69312286376953\n",
      "epoch 88 training_loss 5.016578674316406 validation_loss 102.69226837158203\n",
      "epoch 88 training_loss 5.121766090393066 validation_loss 102.73853302001953\n",
      "epoch 88 training_loss 5.125708103179932 validation_loss 102.64213562011719\n",
      "epoch 88 training_loss 5.241564750671387 validation_loss 102.65367126464844\n",
      "epoch 88 training_loss 5.226656436920166 validation_loss 102.6853256225586\n",
      "epoch 88 training_loss 5.3673601150512695 validation_loss 102.68480682373047\n",
      "epoch 88 training_loss 4.720218181610107 validation_loss 102.75415802001953\n",
      "epoch 88 training_loss 4.840436935424805 validation_loss 102.76438903808594\n",
      "epoch 88 training_loss 4.530788421630859 validation_loss 102.76333618164062\n",
      "epoch 88 training_loss 5.499783515930176 validation_loss 102.73300170898438\n",
      "epoch 88 training_loss 4.826775074005127 validation_loss 102.76782989501953\n",
      "epoch 88 training_loss 4.728724479675293 validation_loss 102.6986083984375\n",
      "epoch 88 training_loss 5.6741557121276855 validation_loss 102.63382720947266\n",
      "epoch 88 training_loss 4.687510967254639 validation_loss 102.61931610107422\n",
      "epoch 89 training_loss 4.736007213592529 validation_loss 102.63015747070312\n",
      "epoch 89 training_loss 4.9303975105285645 validation_loss 102.61621856689453\n",
      "epoch 89 training_loss 4.510044574737549 validation_loss 102.6317138671875\n",
      "epoch 89 training_loss 5.003666400909424 validation_loss 102.62615966796875\n",
      "epoch 89 training_loss 5.029989719390869 validation_loss 102.66703033447266\n",
      "epoch 89 training_loss 5.271913051605225 validation_loss 102.6515121459961\n",
      "epoch 89 training_loss 4.708912372589111 validation_loss 102.68829345703125\n",
      "epoch 89 training_loss 5.142703056335449 validation_loss 102.67133331298828\n",
      "epoch 89 training_loss 5.325555324554443 validation_loss 102.67591857910156\n",
      "epoch 89 training_loss 5.0722808837890625 validation_loss 102.67862701416016\n",
      "epoch 89 training_loss 4.86509895324707 validation_loss 102.74688720703125\n",
      "epoch 89 training_loss 5.146024703979492 validation_loss 102.70443725585938\n",
      "epoch 89 training_loss 5.580873966217041 validation_loss 102.71514129638672\n",
      "epoch 89 training_loss 5.038760662078857 validation_loss 102.70478057861328\n",
      "epoch 89 training_loss 5.162599563598633 validation_loss 102.6944351196289\n",
      "epoch 89 training_loss 5.245211124420166 validation_loss 102.69952392578125\n",
      "epoch 90 training_loss 4.873018741607666 validation_loss 102.71306610107422\n",
      "epoch 90 training_loss 5.063167572021484 validation_loss 102.72654724121094\n",
      "epoch 90 training_loss 5.098832130432129 validation_loss 102.71802520751953\n",
      "epoch 90 training_loss 5.069820880889893 validation_loss 102.70112609863281\n",
      "epoch 90 training_loss 5.208840847015381 validation_loss 102.69287872314453\n",
      "epoch 90 training_loss 4.882936477661133 validation_loss 102.7232894897461\n",
      "epoch 90 training_loss 5.100897789001465 validation_loss 102.68383026123047\n",
      "epoch 90 training_loss 5.208045482635498 validation_loss 102.62606048583984\n",
      "epoch 90 training_loss 4.878448009490967 validation_loss 102.66358184814453\n",
      "epoch 90 training_loss 4.971875190734863 validation_loss 102.69307708740234\n",
      "epoch 90 training_loss 5.266717433929443 validation_loss 102.66801452636719\n",
      "epoch 90 training_loss 5.120240211486816 validation_loss 102.67796325683594\n",
      "epoch 90 training_loss 4.892885684967041 validation_loss 102.70315551757812\n",
      "epoch 90 training_loss 4.8068108558654785 validation_loss 102.689697265625\n",
      "epoch 90 training_loss 5.064692974090576 validation_loss 102.67302703857422\n",
      "epoch 90 training_loss 5.2645487785339355 validation_loss 102.69145965576172\n",
      "epoch 91 training_loss 5.116893768310547 validation_loss 102.67644500732422\n",
      "epoch 91 training_loss 4.7821760177612305 validation_loss 102.70909118652344\n",
      "epoch 91 training_loss 4.86320161819458 validation_loss 102.69747161865234\n",
      "epoch 91 training_loss 4.805322647094727 validation_loss 102.71442413330078\n",
      "epoch 91 training_loss 4.945176601409912 validation_loss 102.7364273071289\n",
      "epoch 91 training_loss 5.051576137542725 validation_loss 102.721435546875\n",
      "epoch 91 training_loss 4.772139549255371 validation_loss 102.70850372314453\n",
      "epoch 91 training_loss 5.419373989105225 validation_loss 102.66616821289062\n",
      "epoch 91 training_loss 5.445723056793213 validation_loss 102.70043182373047\n",
      "epoch 91 training_loss 4.998384475708008 validation_loss 102.71879577636719\n",
      "epoch 91 training_loss 4.7448225021362305 validation_loss 102.72723388671875\n",
      "epoch 91 training_loss 4.933589935302734 validation_loss 102.74468231201172\n",
      "epoch 91 training_loss 5.344546318054199 validation_loss 102.67034912109375\n",
      "epoch 91 training_loss 5.039802074432373 validation_loss 102.68997955322266\n",
      "epoch 91 training_loss 5.163887977600098 validation_loss 102.63028717041016\n",
      "epoch 91 training_loss 5.438440799713135 validation_loss 102.60163879394531\n",
      "epoch 92 training_loss 4.819093704223633 validation_loss 102.60281372070312\n",
      "epoch 92 training_loss 5.2210493087768555 validation_loss 102.6498794555664\n",
      "epoch 92 training_loss 4.622713565826416 validation_loss 102.65238952636719\n",
      "epoch 92 training_loss 4.987256050109863 validation_loss 102.6419677734375\n",
      "epoch 92 training_loss 5.260201454162598 validation_loss 102.70103454589844\n",
      "epoch 92 training_loss 4.875545978546143 validation_loss 102.7083969116211\n",
      "epoch 92 training_loss 5.175056457519531 validation_loss 102.68645477294922\n",
      "epoch 92 training_loss 4.909595966339111 validation_loss 102.67140197753906\n",
      "epoch 92 training_loss 5.073139667510986 validation_loss 102.67234802246094\n",
      "epoch 92 training_loss 5.092608451843262 validation_loss 102.70045471191406\n",
      "epoch 92 training_loss 5.143118381500244 validation_loss 102.69232177734375\n",
      "epoch 92 training_loss 5.00986909866333 validation_loss 102.66891479492188\n",
      "epoch 92 training_loss 5.014524459838867 validation_loss 102.64383697509766\n",
      "epoch 92 training_loss 5.070064544677734 validation_loss 102.67504119873047\n",
      "epoch 92 training_loss 5.114981174468994 validation_loss 102.64283752441406\n",
      "epoch 92 training_loss 5.509074687957764 validation_loss 102.65473937988281\n",
      "epoch 93 training_loss 4.614593982696533 validation_loss 102.67003631591797\n",
      "epoch 93 training_loss 4.983635425567627 validation_loss 102.6742172241211\n",
      "epoch 93 training_loss 5.373145580291748 validation_loss 102.70292663574219\n",
      "epoch 93 training_loss 5.048327922821045 validation_loss 102.66548919677734\n",
      "epoch 93 training_loss 5.022930145263672 validation_loss 102.64266967773438\n",
      "epoch 93 training_loss 5.463692665100098 validation_loss 102.6246566772461\n",
      "epoch 93 training_loss 5.598475933074951 validation_loss 102.64539337158203\n",
      "epoch 93 training_loss 4.918604850769043 validation_loss 102.68472290039062\n",
      "epoch 93 training_loss 4.880979537963867 validation_loss 102.71281433105469\n",
      "epoch 93 training_loss 4.7948899269104 validation_loss 102.7069320678711\n",
      "epoch 93 training_loss 4.909404754638672 validation_loss 102.71391296386719\n",
      "epoch 93 training_loss 4.882011413574219 validation_loss 102.71739196777344\n",
      "epoch 93 training_loss 5.185397624969482 validation_loss 102.69774627685547\n",
      "epoch 93 training_loss 5.0975518226623535 validation_loss 102.67420959472656\n",
      "epoch 93 training_loss 4.879065990447998 validation_loss 102.66838073730469\n",
      "epoch 93 training_loss 4.830126762390137 validation_loss 102.6165771484375\n",
      "epoch 94 training_loss 4.845893383026123 validation_loss 102.65564727783203\n",
      "epoch 94 training_loss 5.090180397033691 validation_loss 102.63472747802734\n",
      "epoch 94 training_loss 5.327671527862549 validation_loss 102.67189025878906\n",
      "epoch 94 training_loss 4.927868366241455 validation_loss 102.65433502197266\n",
      "epoch 94 training_loss 5.325639724731445 validation_loss 102.63416290283203\n",
      "epoch 94 training_loss 5.337789535522461 validation_loss 102.6170425415039\n",
      "epoch 94 training_loss 4.601314067840576 validation_loss 102.63443756103516\n",
      "epoch 94 training_loss 4.9276652336120605 validation_loss 102.69473266601562\n",
      "epoch 94 training_loss 5.046341419219971 validation_loss 102.7291259765625\n",
      "epoch 94 training_loss 5.227814674377441 validation_loss 102.72886657714844\n",
      "epoch 94 training_loss 4.751234531402588 validation_loss 102.7321548461914\n",
      "epoch 94 training_loss 5.057726860046387 validation_loss 102.69812774658203\n",
      "epoch 94 training_loss 5.048934459686279 validation_loss 102.70649719238281\n",
      "epoch 94 training_loss 5.091430187225342 validation_loss 102.66659545898438\n",
      "epoch 94 training_loss 4.973058223724365 validation_loss 102.64095306396484\n",
      "epoch 94 training_loss 4.987369060516357 validation_loss 102.6240005493164\n",
      "epoch 95 training_loss 4.957089424133301 validation_loss 102.64264678955078\n",
      "epoch 95 training_loss 5.146117687225342 validation_loss 102.65279388427734\n",
      "epoch 95 training_loss 4.93214225769043 validation_loss 102.69316101074219\n",
      "epoch 95 training_loss 4.828151226043701 validation_loss 102.69332885742188\n",
      "epoch 95 training_loss 5.130650997161865 validation_loss 102.7022705078125\n",
      "epoch 95 training_loss 4.7166748046875 validation_loss 102.68218994140625\n",
      "epoch 95 training_loss 5.241783142089844 validation_loss 102.66741943359375\n",
      "epoch 95 training_loss 5.098739147186279 validation_loss 102.69355773925781\n",
      "epoch 95 training_loss 5.063810348510742 validation_loss 102.74848175048828\n",
      "epoch 95 training_loss 5.0552167892456055 validation_loss 102.68867492675781\n",
      "epoch 95 training_loss 5.042907238006592 validation_loss 102.65918731689453\n",
      "epoch 95 training_loss 5.144019603729248 validation_loss 102.6194839477539\n",
      "epoch 95 training_loss 5.025757789611816 validation_loss 102.6393051147461\n",
      "epoch 95 training_loss 5.128140449523926 validation_loss 102.63986206054688\n",
      "epoch 95 training_loss 4.947328090667725 validation_loss 102.6737060546875\n",
      "epoch 95 training_loss 5.26492166519165 validation_loss 102.6232681274414\n",
      "epoch 96 training_loss 4.959185600280762 validation_loss 102.64988708496094\n",
      "epoch 96 training_loss 4.772765636444092 validation_loss 102.66993713378906\n",
      "epoch 96 training_loss 5.097498893737793 validation_loss 102.65554809570312\n",
      "epoch 96 training_loss 5.428351879119873 validation_loss 102.70647430419922\n",
      "epoch 96 training_loss 5.228038311004639 validation_loss 102.72807312011719\n",
      "epoch 96 training_loss 5.152245044708252 validation_loss 102.6495132446289\n",
      "epoch 96 training_loss 4.904424667358398 validation_loss 102.65764617919922\n",
      "epoch 96 training_loss 4.917311668395996 validation_loss 102.67704772949219\n",
      "epoch 96 training_loss 5.033860683441162 validation_loss 102.6821517944336\n",
      "epoch 96 training_loss 4.934422492980957 validation_loss 102.66314697265625\n",
      "epoch 96 training_loss 5.463494300842285 validation_loss 102.62030792236328\n",
      "epoch 96 training_loss 4.9970879554748535 validation_loss 102.61504364013672\n",
      "epoch 96 training_loss 4.850635528564453 validation_loss 102.62056732177734\n",
      "epoch 96 training_loss 4.820215225219727 validation_loss 102.65637969970703\n",
      "epoch 96 training_loss 5.159809112548828 validation_loss 102.68583679199219\n",
      "epoch 96 training_loss 4.598665237426758 validation_loss 102.68762969970703\n",
      "epoch 97 training_loss 4.926107406616211 validation_loss 102.69956970214844\n",
      "epoch 97 training_loss 5.259765148162842 validation_loss 102.70684814453125\n",
      "epoch 97 training_loss 4.933280944824219 validation_loss 102.7124252319336\n",
      "epoch 97 training_loss 4.805385112762451 validation_loss 102.71859741210938\n",
      "epoch 97 training_loss 4.881205081939697 validation_loss 102.69324493408203\n",
      "epoch 97 training_loss 5.27049446105957 validation_loss 102.64694213867188\n",
      "epoch 97 training_loss 4.957418441772461 validation_loss 102.6435546875\n",
      "epoch 97 training_loss 4.834089279174805 validation_loss 102.64962005615234\n",
      "epoch 97 training_loss 5.215263843536377 validation_loss 102.6479263305664\n",
      "epoch 97 training_loss 4.854282379150391 validation_loss 102.65843963623047\n",
      "epoch 97 training_loss 5.050753116607666 validation_loss 102.62718963623047\n",
      "epoch 97 training_loss 5.274173259735107 validation_loss 102.64682006835938\n",
      "epoch 97 training_loss 4.996561527252197 validation_loss 102.67706298828125\n",
      "epoch 97 training_loss 4.851659774780273 validation_loss 102.73131561279297\n",
      "epoch 97 training_loss 5.450865745544434 validation_loss 102.67401123046875\n",
      "epoch 97 training_loss 4.962606906890869 validation_loss 102.64683532714844\n",
      "epoch 98 training_loss 4.959455966949463 validation_loss 102.72761535644531\n",
      "epoch 98 training_loss 5.403681755065918 validation_loss 102.69473266601562\n",
      "epoch 98 training_loss 5.064610481262207 validation_loss 102.68612670898438\n",
      "epoch 98 training_loss 5.034082889556885 validation_loss 102.68470001220703\n",
      "epoch 98 training_loss 4.91424560546875 validation_loss 102.68637084960938\n",
      "epoch 98 training_loss 5.038896083831787 validation_loss 102.70698547363281\n",
      "epoch 98 training_loss 4.740906715393066 validation_loss 102.69428253173828\n",
      "epoch 98 training_loss 5.186491966247559 validation_loss 102.63960266113281\n",
      "epoch 98 training_loss 4.964837551116943 validation_loss 102.6846923828125\n",
      "epoch 98 training_loss 5.13852596282959 validation_loss 102.70769500732422\n",
      "epoch 98 training_loss 4.894253253936768 validation_loss 102.67098999023438\n",
      "epoch 98 training_loss 5.159879684448242 validation_loss 102.5992202758789\n",
      "epoch 98 training_loss 5.150777816772461 validation_loss 102.59720611572266\n",
      "epoch 98 training_loss 5.045926094055176 validation_loss 102.6357192993164\n",
      "epoch 98 training_loss 4.94404411315918 validation_loss 102.62398529052734\n",
      "epoch 98 training_loss 4.745995998382568 validation_loss 102.62683868408203\n",
      "epoch 99 training_loss 4.838505744934082 validation_loss 102.66744995117188\n",
      "epoch 99 training_loss 5.135776042938232 validation_loss 102.65032196044922\n",
      "epoch 99 training_loss 4.75718879699707 validation_loss 102.62957000732422\n",
      "epoch 99 training_loss 5.325470924377441 validation_loss 102.6515121459961\n",
      "epoch 99 training_loss 5.2046217918396 validation_loss 102.62698364257812\n",
      "epoch 99 training_loss 5.057532787322998 validation_loss 102.6232681274414\n",
      "epoch 99 training_loss 5.361067771911621 validation_loss 102.637451171875\n",
      "epoch 99 training_loss 5.092849254608154 validation_loss 102.65841674804688\n",
      "epoch 99 training_loss 4.884299278259277 validation_loss 102.64126586914062\n",
      "epoch 99 training_loss 4.981810569763184 validation_loss 102.67037963867188\n",
      "epoch 99 training_loss 4.458677768707275 validation_loss 102.68508911132812\n",
      "epoch 99 training_loss 5.116950511932373 validation_loss 102.68222045898438\n",
      "epoch 99 training_loss 4.892716884613037 validation_loss 102.6866226196289\n",
      "epoch 99 training_loss 5.134361743927002 validation_loss 102.69906616210938\n",
      "epoch 99 training_loss 5.11854887008667 validation_loss 102.65558624267578\n",
      "epoch 99 training_loss 5.411442279815674 validation_loss 102.66410064697266\n"
     ]
    }
   ],
   "source": [
    "!python3 extracredit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened?  Well, if you open `extracredit.py`  you will find these lines:\n",
    "\n",
    "```python\n",
    "    # Well, you might want to create a model a little better than this...\n",
    "    model = torch.nn.Sequential(torch.nn.Flatten(),torch.nn.Linear(in_features=8*8*15, out_features=1))\n",
    "\n",
    "    # ... and if you do, this initialization might not be relevant any more ...\n",
    "    model[1].weight.data = initialize_weights()\n",
    "    model[1].bias.data = torch.zeros(1)\n",
    "\n",
    "    # ... and you might want to put some code here to train your model:\n",
    "    trainset = ChessDataset(filename='extracredit_train.txt')\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=True)\n",
    "    for epoch in range(100):\n",
    "        for x,y in trainloader:\n",
    "            pass # Replace this line with some code that actually does the training\n",
    "\n",
    "    # ... after which, you should save it as \"model_ckpt.pkl\":\n",
    "    torch.save(model, 'model_ckpt.pkl')\n",
    "```\n",
    "\n",
    "The `torch.nn.Sequential` line has flattened the input board embedding, and then multiplied it by a matrix.  The function `initialize_weights()` initializes that matrix to be exactly equal to the weights used in the PyChess linear heuristic.  If you look closely at the training part of the code, you will see that it has done nothing at all; this part of the code is only here to show you how the ChessDataset and DataLoader can be used.  After the `pass`, you see that the model, initialized but not trained, has been saved to `model_ckpt.pkl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.C. Leaderboard\n",
    "\n",
    "Now that you've saved `model_ckpt.pkl`, you can score it using `extracredit_grade.py`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siriux/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/2023-2024 Spring/ECE 448/MP/mp07/extracredit_embedding.py:107: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400235349/work/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  self.embeddings = torch.tensor(embeddings,dtype=DTYPE,device=DEVICE)\n",
      "{\n",
      "  \"tests\": [\n",
      "    {\n",
      "      \"name\": \"validoreval_winratio_gt_0.5\",\n",
      "      \"score\": 8.399999999999995,\n",
      "      \"max_score\": 10\n",
      "    }\n",
      "  ],\n",
      "  \"visibility\": \"visible\",\n",
      "  \"execution_time\": 0.2015690803527832,\n",
      "  \"score\": 8.399999999999995,\n",
      "  \"leaderboard\": [\n",
      "    {\n",
      "      \"name\": \"winratio_evaluation\",\n",
      "      \"value\": -1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"winratio_validation\",\n",
      "      \"value\": 0.564\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Time\",\n",
      "      \"value\": 0.2015690803527832\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python3 extracredit_grade.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `leaderboard` section shows two separate scores:\n",
    "\n",
    "- `winratio_validation` is the fraction, of the 1000 boards in `extracredit_validation.txt`, in which your neural net beat the PyChess heuristic.\n",
    "- `winratio_evaluation` is the same thing, but for the boards in `extracredit_evaluation.txt` which you shouldn't have access to.  This is used for final grading with the autograder and will always be -1 locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.D. Extra Credit Grade\n",
    "\n",
    "Your extra credit grade is given by the variable `score` in the output from `extracredit_grade.py`.  It is calculated as\n",
    "\n",
    "```python\n",
    "    score =(2 * (grade_ratio >= 0.5)) + 100 * min(0.08, max(grade_ratio - 0.5, 0)) \n",
    "```\n",
    "\n",
    "where grade_ratio is either winratio_validation or winratio_evaluation depending on whether you are grading locally or on gradescope. You will get 2 points for getting winratio = 0.5 and full 10 points if you can get winratio higher than 0.58.\n",
    "\n",
    "Note: Final grade is determined by the score on gradescope which will be different from local score due to the difference in validation and evaluation set. Local score is for your reference only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.E. Submission Instructions\n",
    "\n",
    "Your file `extracredit.py` must create a pytorch `torch.nn.Module` object, and then save it in a file called `model_ckpt.pkl`.\n",
    "\n",
    "Pre-trained models (of any interestingly large size) cannot be uploaded to Gradescope, so your model will have to be created, trained, and saved by the file `extracredit.py`.\n",
    "\n",
    "You are strongly encouraged to load `extracredit_train.txt` and/or `extracredit_validation.txt` in your `extracredit.py`  function.\n",
    "\n",
    "In order to allow you to train interesting neural nets, we've set up Gradescope to allow you up to 40 minutes of CPU time (on one CPU).  It is possible to get full points for this extra  credit assignment in three or four minutes of training, but some of you may want to experiment with bigger models.\n",
    "\n",
    "#### Warning: Hard to beat alpha-beta!\n",
    "\n",
    "The extra credit thresholds are set so that you can get all of the points if you do well enough on the training data, even if your result is massively overtrained and doesn't generalize well to validation or evaluation data.  That's because it's actually really hard to find a neural model that beats alpha-beta by any significant margin.  <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\">Deep Blue</a> did not use a neural network explicitly, though it had a number of parameters in its evaluation function that were trained using machine-learning-like techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grade'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Grade your homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the main part of this assignment by uploading `submitted.py` to Gradescope. You can upload other files with it, but only `submitted.py` will be retained by the autograder. \n",
    "\n",
    "Submit the extra credit part of this assignment by uploading `extracredit.py` to Gradescope. You can upload other files with it, but only `extracredit.py` will be retained by the autograder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
